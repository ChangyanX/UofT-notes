---
tags: 
 - "mergesort"
 - "master_theorem"
 - "recurrences"
---
# 2018-10-11

Upper bound on \(T(n)\) has a problem, when we try to prove

\(P(n): T(n) \leq c \log_2{n-1}\) for some \(c \in \mathbb{N}\).

When we tried to show \(\log_2{n+1} \leq \log_2{n} + 1\), we arrive at another problem.

We want to use the non-decreasing property of time functions to solve a class of algorithms. However, when we use this property, we **do not use induction**.

Consider *mergesort*.

```python
mergesort(A, b, e):
  if b == e:
      return # time of c
  m = (b + e) // 2 # Time of c2
  mergesort(A, b, m)
  mergesort(A, m+1, e)
  # merge sorted A[b..m] and A[m+1..e] back into A[b..e]
  B = A[:] # copy A # Time of c3 * n: n = e-b + 1
  c = b
  d = m+1
  for i in [b,...,e]: #time of c4 * n
      if d > e or (c <= m and B[c] < B[d]):
          A[i] = B[c]
          c = c+1
      else: # d <= e and (c > m or B[c] >= B[d])
         A[i] = B[d] 
```

Consider the constant time operations as `c`, and the linear time operations as taking time `n`.

Then, assume that the time function of this function \(T\) to be nondecreasing. We need to show that \(b - m + 1 = \lceil{\frac{n}{2}}\rceil\)
and \(e -(m+1) + 1 = \lfloor \frac{n}{2}\rfloor\).

Then 

\[
T = \begin{cases} c & n=1\\T(\lceil{\frac{n}{2}\rceil} + T(\lfloor{\frac{n}{2}\rfloor}) + n & n > 1\end{cases}
\]

Suppose \(n = 2^k\) for some \(k \in \mathbb{N}, k > 0\). Then we can simplify \(T\) as \(T = 2T(\frac{2^k}{2}) + 2^k\).

Unwinding this equation eventually allows us to write \(T\) as a closed form equation with some intuiton (**not a proof**)

\[2^k \cdot T(2^{k-k}) + k \cdot 2^k\]

And thus

\[
2^k \cdot c + k \cdot 2^k
\]

Since \(n = 2^k\)

\[
cn + n\log_2{n}
\]

We now need to prove that \(T\) is non decreasing.

Define \(n^* = 2^{\lceil\log_2 n\rceil}\), or the next largest power of two. Also note that \(\frac{n^*}{2} <n \leq n^* < 2n\).

Note then that, for example, 5*, 6*, 7*, 8* = 8.

We know exactly what \(T\) is on powers of two, and now we know exactly what \(T\) is on \(n^*\).


Note that we can now bound \(T(n)\) by \(T(\frac{n^2}{2})\), and \(T(\frac{n^*}{2})\).

Note that

\[\frac{cn^*}{2} + \frac{n^*}{2} \log_2{\frac{n^*}{2}} < n < cn^* + n^* \log_2{n^*}\]


Now we will use this to prove \(T \in O(n \log n)\) in the general case.

\(T(n) = T(\lceil{\frac{n}{2}\rceil} + T(\lfloor{\frac{n}{2}\rfloor}) + n\)
**Proof**
> Let \(d = 2(2+c) \in \mathbb{R^+}\). Let \(B \in \mathbb{R^+} = 2\), and \(n \in \mathbb{N} \geq B\).
>
> \[
  \begin{aligned}
  T(n) &\geq T(n^*) \\
  &=n^* \log_2(n^*) + cn^* \\
  &< 2n \log_2 {2n} + 2cn \text { since n* < 2n and log is non decreasing}\\
  &= 2n ( 1 + \log_2{n} + c) \\
  &\leq 2n((1+c) \log_2{n} + \log_2{n}) \text { since } n \geq B \geq 2\\
  &= 2n \log_2{(n)} (2 + c)\\
  &\leq d n \log_2{n}
  \end{aligned}
\]


We conjectured that if \(n = 2^k\), then \(T(n) = n \log_2{n} + c{n}\). 

**Proof**
> Let \(d = 2(2+c) \in \mathbb{R^+}\). Let \(B \in \mathbb{R^+} = 2\), and \(n \in \mathbb{N} \geq B\).
> 
> \[
  \begin{aligned}
  T(n) & \geq T(\frac{n^*}{2}) \text{ since T monotonic} \\
        &=\frac{n^*}{2} \log_2{\frac{n^*}{2}} + c \frac{n^*}{2} \\
        & \geq \frac{n}{2} \log_2{\frac{n}{2}} + c \frac{n}{2} \text{ since } n^* \geq n \\
        &= \frac{n}{2} \log_2{n} - \frac{n}{2} + c \frac{n}{2}
  \end{aligned}
\]

> Now we split the logarithmic "super-linear" part into two pieces, one to overpower the linear terms, as we do not know the value of \(c\).
> 
> \[
  \begin{aligned}
   &= \frac{n}{4} \log_2{n} + \frac{n}{4} \log_2{n} - \frac{n}{2} + c \frac{n}{2}\\
   &\geq \frac{n}{4} \log_2{n}
   \end{aligned}
\]

We arrive at the final equation by fixing \(n \geq 4\), and thus \(c \frac{n}{2} \geq 0\).

In general for divide and conquer

\[
T(n) = \begin{cases} k & n \leq B \\ a_1T(\lceil{\frac{n}{B}}\rceil) +a_2T(\lceil{\frac{n}{B}}\rceil) + f(n) & n > B \end{cases}
\]

Where \(b, k > 0, a_1, a_2 \geq 0, a = a_1 + a_2 > 0\), and \(f(n)\) is the cost of splitting and recombining. We hope \(f(n)\) is a polynomial. 

If \(f\) has \(f \in \theta(n^d)\), or rather \(f\) is polynomial...

Then 

\[
T(n) \in \begin{cases} \theta(n^d) & a < b^d \\ \theta(n^d \log n) & a = b^d \\ \theta(n^{\log_b a}) & a > b^d\end{cases}
\]

Called the master theorem for divide-and-conquer algorithms.

For example, for binary search, \(a = 1, b = 2, d = 0\), and \(1 = 2^0\). Thus \(T \in \theta(n^0 \log n)\).
For mergesort, \(a = 2, b = 2, d = 1\). Thus \(2 = 2^1\), and \(T \in \theta(n^1 \log n)\)

\(b\) is how many parts we divide the problem into, \(a\) is the amount of recursive calls. \(d\) is the degree of the cost of splitting and combining if \(f(n)\) is polynomial.

How do we derive the master theorem?

\(T(n) = \begin{cases} k & n \leq B \\ aT(\frac{n}{b}) + cn^d) & n > B\end{cases}\)

Assume \(n = b^k\) for some natural \(k\).

After unwrapping this equation...

\(a^k T(1) + cn^d \sum_0^{k-1} (\frac{a}{b^d})^i = a^{\log_b{n}} k_0 +  cn^d \sum_0^{k-1} (\frac{a}{b^d})^i =  n^{\log_b{a}} k_0 +  cn^d \sum_0^{k-1} (\frac{a}{b^d})^i\)

Notice that we have a geometric series.  By evaluating the 3 cases of the geometric series, we gain the 3 cases of the master theorem.

However, we must need to show that \(T\) is non decreasing, and extend to all values of \(n\).