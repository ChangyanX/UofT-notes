---
tags: 'master theorem'
---

# 2018-10-18  


Recall the master theorem

\[
T(n) \in \begin{cases} \theta(n^d) & a < b^d \\ \theta(n^d \log n) & a = b^d \\ \theta(n^{\log_b a}) & a > b^d\end{cases}
\]

where 

\(b\) is the number of pieces the problem is divided into
\(a\) the number of recursive calls
\(f\) the cost of splitting and combining, hopefully in \(\theta(n^d)\).

> **Example**
> Multiplying lots of bits
> ```
>     1101
>   x 1011
>  _______
> ```
> 
> We get the following results
> ```
>     1101
>    1101
>   0000
>  1101
> --------
> 1000111 
> ```

Notice that there are \(n\) rowise multiplication, and \(n\) column-wise adds, which is \(\theta(n^2)\).

For a 32 or 64 bit machine, multiplication is fast since it fits into the instruction set.

We can divide and recombine the numbers for large values of \(n\) that could possibly not have an instruction.

Consider this situation. \(2^n = n\) left shifts, and addition/subtraction is in \(\theta(n)\).


```
  11|01
  -----
x 10|11
-------
```

We can think of this like so 

\(x = (1100 + 01) = (11)(2^2) + 01\)
\(y = (1000 + 11) = (10)(2^2) + 11\)

\(xy = (2^{n/2}x_1 + x_0)(2^{n/2}y_1 + y_0)\)
\(xy = 2^nx_1y_1 + 2^{n/2}(x_1y_0 + y_1x_0) + x_0y_0\)

Four multiplications of \(\frac{n}{2}\) numbers and some addition.

So, compare \(n\) \(n\)-bit addition versus
1. divide each factor in (roughly) half (\(b = 2)\)
2. multiply the halves (recursively if too big) \((a = 4)\)
3. combine products with shifts and adds \(f \in \Theta(n^1)\)

Since \(4 = a > b = 2\), by the master theorem, this algorithm is going to be \(\theta(n^{\log_b(a)}) = \theta(n^2)\) â˜¹

However look at Gauss' trick

\(xy = 2^nx_1y_1 + 2^{n/2}x_1y_1 + 2^{n/2}((x_1-x_0)(y_0 - y_1) + x_0y_0) + x_0y_0\)

Notice that we've saved one multiplication and added some addition and subtraction
1. divide each factor in (roughly) half (\(b = 2)\)
2. subtract the halves
3. multiply the halves (recursively if too big) \((a = 3)\)
4. combine products with shifts and adds \(f \in \Theta(n^1)\)

Thus by the master theorem, this gives us
\(\theta(n^{\log_2(3)})\)

### Points

Consider a set of points

\(P = \{(x_0, y_0), (x_1, y_1), ..., (x_n, y_n)\)

Then a vertical line across these set of points. How do we find the points closest to the line?

Brute force \(\in \Theta({n \choose 2}) = \Theta(n^2)\) to find the distances of every point.
For divide and conquer

\[
T(n) = \begin{cases} k & n \geq 3 \\ T(\lceil\frac{n}{2}\rceil) +  T(\lfloor\frac{n}{2}\rfloor) + f(n) & n^d \end{cases}
\]

Now we have

\(b, a = 2\), unknown \(d\).

There is an \(n \log_2 n\) algorithm for this.

If \(P\) is a set of points
1. Construct and sort \(P_x\), \(P_y\), which costs \(n \log n\).
2. For each recursive call, construct ordered \(L_x, L_y, R_x, R_y\).
3. Recursively find closest pairs \((l_0, l_1)\) \((r_0, r_1)\) with minimum distance \(\delta\).
   A remarkable result is that there are at most 7 points with \(\geq y\) in the \(d\) neighbourhood ofthe veritcal line for each point.
4. \(V\) is the vertical line splitting \(L\) and \(R\), \(D\) is the \(\delta\)-neighbourhood of \(V\), and \(D_y\) is ordered by \(y\)-co-ordinate
5. Traverse \(D_y\) looking for minimum pairs 7 places apart.
6. Choose minimum pair from \(D_y\) versus \((l_0, l_1), (r_0, r_1)\).

Hence
\(a = b = 2\), \(d = 1\). Since the sorting step takes \(n \log n\) and by the master theorem the algorithm takes \(n \log n\), the algorithm takes \(n \log n\).

## Correctness

Consider recursive binary search

```python
n: len(A) = e-b+1
def recBinSearch(x: val, A: val[]@sorted, b: index@begin, e:index@end):
  if b == e:
      if x <= A[b]:
        return b
      else:
        return e + 1
```

We want to establish some conditions

```typescript
- val typeof Comparable
- 0 <= b <= e <= len(A)
- A[b...e] is sorted non-decreasing
```

Then `recBinSearch` terminates and returns index \(p\)

> \(b \leq p \leq e + 1\)
> 
> \(b < p \implies A[p-1]\)
> 
> \(p \geq e \implies x \geq A[p]\)

> Precondition \(\implies\) termination and post condition
> 
> Proof: induction on \(n = e - b + 1\)
> **Case \(n =1)**
> 
> There are no loops or further calls. Hence \(p = b = e \iff x \geq A[b = p]\), or \(p = b + 1 = e + 1 \iff x > A[b = p - 1]\), so post condition satisfied.
> This choice forces if and only if.
> 
> **Induction Step**
> Assume \(n > 1\)  and that the post condition is satisfied for inputs of size \(1 \geq k < n\)
> 
> **Case \(x \geq A\)**:
> 
> We need to show that IH applies to RBS(x, A, b, m)
> Translate the postcondition to RBS(x, A, b, m)
>
> 1. \(b \leq p \leq m+1\)
> 2. \(b < p \implies A[p-1] < x\)
> 3. \(p \leq m \implies A[p] > x\)
> 
> Then we show that RBS(x, A, b, e) satisfies the post condition
> 1. \(b \leq p \leq m + 1 \leq e + 1\) by IH and m < e
> 2. \(b < p \implies A[p-1] < x\)
> 