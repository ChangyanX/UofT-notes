# 2019-11-13

* Better page table designs
  * Memory required for linear page table can be large
  * need one PTE for page.
* How to reduce overhead?
  * Only a small portion of the address space is being used
  * Only map what is being used.
* How to map only used memory?
  * Use 3 page tables -- one per logical segment
  * Need:
    * Base register (where PT starts in physram)
    * Bound/limit register
    * 3 pairs of base/bound registers
  * Big memory savings
  * End up with lots of page table waste
    * large but sparse heap
  * External fragmenttion, page tables can be of arbitrary size
* Linear page table is just array indexed by page number
* Multilevel page table
  * split page table into pages of PTE
  * Use a page table directory
  * similar to inodes
* PTBR -- Page Table Base Register
* virtaddrs have 3 parts
  * master page table maps VA to secondary page table
  * secondary page table maps page number to physical frame
  * offset selects address within physical frame
  * **each address requires 3 memory accesses!!**
* 32-bit virtual address space
  * 4K pages, 4 bytes/PTE
  * How many bits in offset? 4K = 12 bits. leaves 20 bits
  * Master Page Table occupies 1 page frame, each secondary page table 1 page frame.
  * Each page table entry occupies 4 bytes, how many entries can we fit in a frame?
* This saves space, but adds more levels of indirection
* 64 Bit address space only uses 48 bits. We don't need any more let, and can extend later to 64 bits without breaking compatibility.
* Suppose we just extend the hierarchical page tables with more levels?
  * On solution is hashed page tables that maps virtual page numbers in fixed sized hash tables
  * search entries at that bucket for matching VPN.
* **Inverted page tables**
  * One page table for whole system
    * Entry for each physical page frame
    * entries record which virtual page # is stored in that frame
    * need to record process id too
  * Less space but lookups are slower
  * References use virtaddr, table indexed by physaddr
  * Use hashing to reduce search time
* Translation Lookaside Buffer
  * Caches page table entries
  * hardware cache stored right into CPU
  * stored recently used page table entries
  * fully associative cached
  * cache tags are virtual page numbers
  * cache values are page table entries, so we can combine with offset
* Processes only use a handful of pages.
  * Even with a 64 entry TLB that maps 64 pages (256K), 99% translations hits TLB.
  * We still need to deal with TLB miss
  * We are often accessing data close together.
* every context switch, we flush the TLB