# 2019-11-13

* Remember, rolling forward the stationary distribution one more time unit, we get the same distribution.
* Imagine the world progressing according to some markov chain, but you can observe the outputs (effects) at each time step. 
* We don't ever observe the hidden state, only the effects.

* For example, we can observe if people are using umbrellas, but not if its raining.
  * Initial distribution \(Pr(X_1)\)
  * Transitions \(Pr(X_t | X_{t - 1})\)
  * Emissions \(Pr(E_t | X_t)\)
* These assumptions are all independent. Given \(X_i\), \(E_i\) is independent of everything else!
* That is, \(Pr(E_t | X_t, E_{t-1}, ...) = Pr(E_t | X_t)\).
* The past is independent of the future, and vice versa given the current state. The current state d-separates the past and future.
* If we don't know any evidence, then there is a path. It is not independent unless we know an intermediary state.
* HMMs can model sequences of observations and get effects.

* Filtering or monitoring is the task of tracking the probability of the current state given all of the evidence. \(Pr(X_t | e_1, ..., e_t)\). 
  * We know the initial distribution \(P(X_1)\). Usually we just assume it's uniform, and over time the initial state matters less and less; that is \(Pr(X = d)\) is uniform.
* As time passes we get observations and we update our distribution over \(X\).
  * Move from \(Pr(X_{t-1}) | e_{t-1}, e_{t-2}, ..., e_1)\) to \(Pr(X_t | e_t, ..., e_1)\).
* We can use variable elimination to compute \(Pr(X_t | e_t, e_{t = 1}, ..., e_1)\) at all points \(t\). But we can re use some knowledge to solve this more efficiently
* Relevance reasoning shows that all future variables \(X_t, E_t, ...\) are irrelevant if we want to find \(Pr(X_{t - 1} | e_{t-1})\). Only the past is relevant.
  * We then use VE to solve the past
  * Then \(Pr(X_{t - 1} | e_{t-1}, ..., e_1) = normalize(Pr(e_{t-1}|X_{t-1} F_{t-1}(X_{t-1}))\)
  * Now that we know that we obtain a factor for \(X_t\). 