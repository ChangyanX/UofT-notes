# Algorithms for Search

* Output is a sequence of actions that transform the initial state to a state satisfying the goal test.
  * or the sequence of states that arise from these actions

* Sequences may be optimal for cost or for length, or no guarantees.

## Paths
A *path* in the search space starts at the initial state, and transitions by actions to a sequence of states. 

* A *node* is a path, each node \(n\) is a path in th search space. 
  * The path might be stored as a pointer from a node data structure.
  * `n.state()` returns the terminal state of the path `n`
  * `n = <A, B, C>; n.state = C`
  * The *open* set or list are nodes that start at the initial start state.
  * The set of all nodes for a search space is the set of all possible paths.
  * Consider \(n = <a, b, c, d>\), then \(state(n) = d\). Let \(S(d) = \{e, f\}\). Then we have \(n_e = <a, b, c, d, e>\), and \(n_f = <a, b, c, d, f>\).
  * If we explored all paths and none are empty, this problem is unsolvable.

We can change `open` in different ways and get different behaviours.

* Uninformed search techniques
  * Breadth-first
  * Uniform-cost
  * Depth-first
  * Depth-limited
  * Iterative deepening

In search, we represent *graph nodes* as state space states, and graph edges and state spaces actions. However, we don't have the explicit graph because our search space may be exponentially big.

BFS implementation for water jugs. We start at (0, 0), goal is (*, 2). Placing new paths that extend the current path at the end of open, and we extract the first element. Italics is next expansion, and bolded is newly added.

1. OPEN = {*<(0, 0)>*}
2. OPEN = {*<(0, 0), (3, 0)>*, **<(0, 0), (0, 4)>**}
3. OPEN = {*<(0,0),(0,4)>*, **<(0,0),(3,0),(0,0)>, <(0,0),(3,0),(3,4)>, <(0,0),(3,0),(0,3)**>
4. {*<(0,0),(3,0),(0,0)>*, <(0,0),(3,0),(3,4)>, <(0,0),(3,0),(0,3)>, **<(0,0),(0,4),(0,0)>, <(0,0),(0,4),(3,4)>, <(0,0),(0,4),(3,1)>**}

What is the time complexity?

Let \(b\) be the maximum number of successors of any node (maximal branching factor).
Let \(d\) be the depth of the shortest solution. The root at depth 0 is a path of length 1, so the length of the path is \(d + 1\).

Since BFS finds the shortest solution we don't need to consider anything past \(d\).

The time is then 

\(1 + b + b^2 + b^3 + ... + b^{d- 1} + b^d + b(b^d - 1) = \mathcal{O}(b^{d+1})\).

We don't need to expand the last node, so we have \(b(b^d - 1)\). 

The space capacity is the same, because if the goal node is the last node at \(d\), all of the successors of the other nodes will be in OPEN when the goal node is expanded. **We need to expand the next level in order to find the final node**.

```
   X
  / \
 /   \
 A B C
/\ /\
```

We need to expand `A` and `B` before getting to `C` and testing its goal.

Completeness is 
  * if each transition has costs \(\geq \epsilon > 0\).
  * We need to bound the length of the path, otherwise we could have an infinite path with finite cost. In other words, **every path must grow in cost**.
  * Because the costs increases more and more, an infinite path will eventually lose out to a finite length path. 

Optimality

**Lemma 1**
Let \(c(n)\) be the cost of node \(n\) on OPEN. Then, if \(n_2\) and \(n_1\) are nodes and \(n_2\) is expanded immediately after \(n_1\), then \(c(n_1) \leq c(n_2)\).

**Proof**
1. If \(n_2\) was on OPEN when \(n_1\) was expanded. We need to **expand the cheapest path** on OPEN. We must have \(c(n_1) \leq c(n_2)\), otherwise \(n_2\) woud have been selected first.
2. If \(n_2\) was added to OPEN, then \(n_2\) is a successor, then \(c(n_2) - c(n_1) < \epsilon \implies c(n_1) < c(n_2)\).

**Lemma 2**
When \(n\) is expanded, every path in the search space with cost strictly less than \(c(n)\) has already been expanded.

**Proof**
* Assume \(n_k = <S, s_1, ..., s_k>\) is a path with cost less than \(c(n)\). We define all prefixes of \(n_k\), such that \(n_0 = <Start>, n_1 = <Start, s_1>, ...\)

* Let \(n_i\) is the last node in the sequence that has already been expanded by search.
* Suppose \(i < k\), then \(n_i\) has been expanded but \(n_{i + 1}\) has not. 
* \(n_{i + 1}\) must still be on OPEN, because \(n_i\) was expanded, and \(n_{i + 1}\) is a successor path.
* By contradiction we have assumed \(c(n_{i + 1}) \leq c(n_k) < c(n)\), since \(n_{i + 1}\) is a subpath of \(n_k\).
* But then we would have expanded \(n_{i + 1}\) before \(n\)! Hence we must have that \(i = k\) and every node before \(n\), including \(n_i\) and \(n_k\) must already be expanded.
* 







