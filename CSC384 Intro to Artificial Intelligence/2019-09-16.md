# # 2019-09-16
* Uniform cost search always returns minimum cost
* In search algo there is a while loop
  * While loop extracts a node from `OPEN`

* Lemma 1
  * \(c(n_i) \leq c(n_{i + 1}\)
  * The cost of the nodes expanded must be at least the cost of the previous one
  * costs can not go down
* Lemma 2
  * Why are we even talking about the minimality of uniform cost search
  * When we expand \(n\) (in the while loop), every single path in the search space with less cost has already been expanded.
  * Think about the start state and a bunch of paths. Let \(n\) be a path from \(S\) to \(A\), and \(n\) has some cost.
    * If there was a path \(n_k\) with cost less than \(n\), we would have already expanded it.
    * How much of \(n_k\) have we expanded? We've gone some way along \(n_k\), then \(n\) is *supposed* to be the cheapest thing on `OPEN`. So if we've gone some way along \(n_k\), and \(n_k\) is not expanded, then it must be on `OPEN`, but then \(n_k\) has cost less than \(n\). It must have been expanded before \(n\).
  * The algorithm only checks if \(n\) is a goal node when you're extracting.
* Lemma 3
  * Let \(A\) be a state, and a bunch of paths (nodes) from \(S\) to \(A\).
  * The first path that expands that leads to \(A\) is the cheapest path.

## Time and Space Complexity
* BFS is exponential is the depth of the cheapest solution, so is uniform cost search.

\[\mathcal{O}(b^{\frac{C*}{\epsilon}})\]

* Suppose \(C^*\) is the cost of the optimal solution, and there may be many paths with costs \(\leq C^*\).
* Paths with cost lower than \(C^*\) can be as long as \(C^*/\epsilon\), since each one could cost \(\epsilon\). 
* > If C is your destination cost and each step gets you ε closer to the goal, the number of steps you need to take is given by C / ε + 1. The reason for the +1 is that you start at distance 0 and end at C / ε, so you take steps at distances

## DFS
* Place new paths that extend the current path at the *front* of OPEN.
* BFS become more expensive with cycles, because you eventually pick a cheaper path
* DFS you could potentially never come back
* Backtrack points remain on OPEN.
* If we have an infinite search space, we have incompleteness.
* If we prune paths with cycles, then we get completeness if we have finite state space.
* If WC, we have tree of path length \(m\), then \(O(b^m)\).
* \(m\) can be much bigger than the depth of the optimal solution.
  * However if there are many solutions it can be much quicker than breadth
* We only have linear space! \(O(bm)\).
  * When we go down the path we have only the unexplored siblings, of which there are \(b\) (branching factor)
  * `OPEN` only contains the deepest node on the current path with the backtrack points.

### Depth Limited Search
* Perform depth first search but only to a pre-specified depth limit \(D\).
* No node representing a path of length more than \(D+1\) can be placed on `OPEN`.
* We truncate the search by looking only at paths of length \(D+1\).
* For DFS, OPEN is a stack! Not a priority queue as in Uniform Cost.
* Implementation is DFS with a cutoff flag that determines if we hit a cutoff. 
* Obviously incomplete
### Iterative Deepening Search
* We want a complete algorithm with DFS space
* Set a limit \(L = 0\). If we fail, then increase \(L\) and try again, and again, until we find a solution, or we exhaust the state space.
* We're doing DFS \(k\) times, saving nothing. Trading space for time.
* We have completeness
  * If there is a solution, it has solution of depth \(d\), we'll get it when \(L = d\).

#### Time complexity of IDS
* Root node examined \(k\) times
* Next level examined \(k-1\) times, and so on so forth
* The nodes at the bottom are only examined once.
* Solution lies at depth \(d\) so we do \(d+1\) searches
* \((d + 1)b^0 + db^1 + (d-1)b^2 + ... + b^d = \mathcal{O}(b^d)\).
* **We only explore the last level once.** This dominates the other complexities.
* Note that for BFS we have complexity dominated by \(O(b^{d+1})\), because we need to generate the successors as we explore the last level. 
* Nodes at the limit are not expanded but bFS needs to expand all nodes.
* We still have linear space complexity, since we throw away the results of bad searches.
* Optimal if the costs are uniform
  * If not, we can bound the cost
    * Only expand paths of cost less than the cost bound
    * Keep track of the minimum cost unexpanded in each DF iteration, increasing the bound.
    * This is minimal but can be more expensive, because the variety of costs you can have can be large. We need as many iterations of search as there are distinct path costs.

## Path Checking
If \(n_k\) is the path \(<s_0, s_1, ..., s_k>\), we expand \(s_k\) to obtain state \(c\), we have \(<s_0, ..., s_k, c>\) as the path to \(c\). Write such paths \(<n, c>\) where \(n\) is the prefix, and \(c\) the final state of the path.

Ensure \(c\) is not equal to the state reached by any ancestor of \(c\). Essentially we need to check if \(c\) is in \(n\), and only add \(<n, c>\) to OPEN if it is not.

Paths are checked in isolation.

## Cycle Checking
We keep track of all states added to OPEN.
* When we expand \(n_k\) to obtain \(c\)
  * ensure \(c\) is not equal to **any previously seen state**
    * Not just the states in \(n_k\).
  * If so, do not add \(<n_k, c>\) to OPEN.
* This is called cycle checking
* Results in exponential space complexity.
* If we want to ensure that we find the cheapest path, we need to store the cost of getting to that state. We don't want to reject \(c\) if the path is cheaper than the previous path.
* With uniform cost search, we do not need to keep track of the cost, because the first time we see a path, it is the cheapest.