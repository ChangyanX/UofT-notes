# 2020-10-09

* What does it mean to compute `overlap`?
  * Cosine Similarity Score
    * cosine of the angle between two vectors (dot product), scaled by the length of the vectors
  * Dice Coefficient
* If we have \(B\), we want to know which \(A\) is most likely
  * take \(\argmax_A P(A|B) = \argmax_A P(B|A) \cdot P(A)\)
  * we only care about the best \(A\), so we don't care about \(P(B)\).
  * not actually used in practice because of other factors 
* Supervised Bayesian methods
  * bayes decision rule
  * classify contexts according to which sense
  * Pick sense \(s_j\) that is most probably in the given context, where \(j = \argmax_j P(s_j|C)\)
  * Want sense \(s\) of word \(w\) in context \(C\) st \(P(s'|C) > P(s_k |C) \forall s_k \neq s'\)
    * probability of the entire context C is only the product of the individual probablities of the words in the context given the sense
    * \(\argmax P(s_k) \Pi_{v_j \in C} P(v_j | s_k)\)
    * assume all the words have nothing to do with the other words in the context -- naive bayes assumption.
  * normalize probability of word sense by the frequency that a word shows up in context
  * To do this we need sense annotated corpora to train a model 
    * expensive, time consuming human work
    * semcor 700K PoS tagged tokens (200K are WordNet sense tagged)
    * singapore DSO corpose 200 interesting word types  in about 2M tokens
  * Systems based on naive Bayes methods have 62-72% accuracy for selected words with adequate training data
    * specific number of word tokens in context that has been challenged to ambiguate
    * face challenge, make answers, and check correct answers
    * by comparison by annotations in training data
  * Neural implementation of Lesk's algorithm
    * instead of word counts, use lexical-semantic vector-space embeddings
      * calculated probably by a language model
    * attention mechanism to distort context-word vectors and dictionary definition vectors with each other
  * Evaluations
    * Lesk's: 50-60%
    * Naive Bayes: 62-72%
    * Neural Lesk: 65%-72%
    * Neural Lesk with 'sentence' instead of word embeddings: 69%-72%
    * Neural hierarchal model with both: 68%-73%
      * Verbs are still tough: 56%-58%
* Decision list: ordered list of strong, specific clues to senses of homonym (Yarowsky 1995)
