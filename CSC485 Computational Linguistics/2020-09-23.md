# 2020-09-23

* constituent is a phrase that is well formed, but has some semantic significance
* NPs are good examples of constituent
* NPs have properties
  * gender
  * number
  * person (most are third, I, you are first and second)
    * speaker or hearer are being referred to
  * case
* NPs can be preceded by adjectives, and followed by PPs as post-modifier
* VPs have properties
* AP
  * we use of to put NP as complement of AP "proud of Kyle"
* PP 
* complementizer packages up a preposition as an NP
* infinitizer "to" forms an infinitive verb from a VP
* clauses can act as NPs
  * "[To become a millionaire by the age of 30] is what Ross wants"
* Gerunds are verbs that can act as NPs
  * in English, nominalizer -ing
* CFG
  * A context-free grammar is a quadruple \(G = (V_t, V_n P, S)\)
    * \(V_t\) is a finite set of terminal symbols
    * \(V_n\) is a finite set of non-terminal ymbols
    * \(P\) is a finite set of production rules of the form \(A \to \alpha\) where \(A \in V_n\) and \(\alpha\) a sequence of symbols in \((V_n \cup V_t)*\)
    * \(S\) is the start symbol in \(V_t\) to represent the whole sentence.
  * \(V_t\) and \(V_n\) can be inferred from \(P\).
* Parsing
  * the process of determining a structure of a sequence of words, given a grammar
  * which grammar rules should be used?
  * which symbols should each rule apply?
  * input:
    * CFG
    * String, or more precisely sets of categories (parts of speech tagging)
      * {noun, verb} {noun, verb} {det} ... etc
      * akin to lexing?
      * PoS can be highly ambiguous
        * time can be N or V
        * time flies like an arrow has six different readings 
    * Process
      * L-to-R, guess how each word fits in
  * gold tags: someone looked at corpus and checked parts of speech tags
    * if input tag sequence is wrong you're SOL
  * backtracking search non-deterministically
    * each guess save state of parse
    * at choice points, statistics guide the search
    * estimated from corpora
    * order of preference
  * top-down or rule-directed parsing
    * can I take rules and match them to input?
      * trees written upside down -- root is at the top
      * initial goal is some \(S\)
      * Repeatedly look for rules that decompose or expand current goals
        * \(S\) decomposes to \(NP\) and \(VP\)
      * Eventually get to goats that look at input
        * \(NP\) may decompose to {det, noun}
      * Succeeds iff entire input stream is accounted for as \(S\).
  * example: recursive descent parser
    * `nltk.app.rdparser()`
    * operations on leftmost frontier node
      * expand it
      * match to next input word
  * phrase structure grammar
  * bottom-up parsing
    * can I take this input and match it to these rules
      * try to find rules that match a possible PoS of rules
      * rules match constituents formed
      * succeed iff entire input stream is accounted for as \(S\).
    * example: shift-reduce parser
      * `ntlk.app.srparser()`
      * shift next input word onto stack
      * match top *n* elements of stack to RHS of rule
      * reduce them to LHS
      * "my dog saw a man in the park with a statue"
        * shift my
        * my -> Det reduce
        * shift dog
        * dog -> N reduce
        * shift N, shift Det
        * N Det -> NP reduce
        * ... etc
        * eventually we recognize a sentence
      * very prone to mistakes. we need statistical preferences to know when to push, and when to shift
      * choice in NLTK:
        * always prefer reduction to shifting
        * choose the first-listed reduction that applies
      * choice real life
        * prefer reduction to shifting but not necessarily for larger constituents
    * top down or bottom up search exploit useful idiosyncracies that CFG rules alone or together often have
      * problem: 
        * need to backtrack, recompute constituents
        * recompute common prefixes
      * solution:
        * keep track of completed constituents
        * keep track of partial rule matches
* Dependency grammar
  * word dependency parsing
  * given PoS-tagged sentence, find dependencies
  * in phrase structure grammar, roots are a category
  * in word dependency, root is a word, the head of the phrase
  * convention that end of sentence marker is ROOT
    * otherwise, head is the verb of the sentence 
  * edges have label in dependency parsers
    * labels often called "grammatical function"
  * temporal propositional phrase modifier is a complement ('in september')
  * MOD means modifier
  * Dependency graphs
    * a dependency structure is a directed graph G
      * \(V\) nodes
      * \(E\) arcs
      * a linear precedence order \(<\) on V
      * number of nodes is just # of words in stings + punctuation
    * Labled graphs
      * nodes in \(V\) are labeled with word forms and annotation
      * arcs in \(E\) labeled with dependency types
    * Conventions \((i, j \in V)\)
      * \(i \to j \equiv (i, j) \in E\): \(i\) is the (direct) head of \(j\).
      * \(i \to *j \equiv (i = j \vee \exists k. i \to k, k \to *j\): \(i\) is a head of \(j\) through multiple jumps