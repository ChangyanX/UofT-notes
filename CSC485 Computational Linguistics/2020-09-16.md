# 2020-09-16

* probablistic models may produce grammatical utterances, but semantically meaningless
  * "colourless green ideas sleep furiously" is syntactic but not interpretable
  * c.f. "furiously sleep ideas green colourless" is ungrammatical
* language models don't assign probablities to sentences
  * tells probability of the next word
  * language models read prefixes, and predict the next word
  * \(W_i = \argmax_w P(w | w_1, ... w_{i-1})\)
  * Example:
    1. Athens is the capital _
    2. Athens is the capital of _
  * What do we need to know to predict (1)? What do we need to know to predict (2)?
  * If we can calculate the conditional probabilities we can get the probability of the entire string.
* point-biserial correlations
  * grammaticality taken to be a binary variable
  * probability produced by a language model for a string of words is continuous
  * how well the continuous model predicts the partial 
  * assessing the quality of the model
  * get a correlation score between 1 and -1
  * grammaticality taken to be a discrete variable
* GPT-2
  * can GPT-2 predict grammaticality?
    * lolno
    * point-biserial does not indicate that GPT-2 can predict grammaticality
  * GPT-2 can be primed
* when people train language models, you train on a corpus of real world text
  * enormous bias towards grammatical input
* deep learning advocates see CL as hunting for "handcrafted" features to improve classifiers
  * before DL, discriminative pattern recognition methods were already in widespread use before DL that had already invalidated this
* DL does add value
  * modularity of different network layers
  * novelty of approach
  * practitioner can produce a baseline system with little expertise
* grammaticality isn't really such a problem though
* More successful methods
  * supervised (trained on curated corpuses)
  * trained on CoLA (split into training, dev, test sets)
  * many use NNs, but not LMs
  * clamp a classifier that take softmax outputs as inputs
  * accuracy around 65-71%
  * Mathhew correlation score 0.253-0.7
  * uniformly guessing grammatical on everything in CoLA has 71% accuracy also 
    * **CoLA is not balanced**
  * use LMs inside as components
  * grammaticality test probably will not come from LMs
    * language is not just a sequence of words
* ngram language model looks at n-1 previous words
  * in terms of state of the art, no one really does this anymore
* grouping refers to how the examples are distributed in the materials that CoLA was collecting grammaticality information
  * small set of similar sentences
* lexical choice was throwing off the grammaticality classifiers
* is grammaticality really a discrete variable?
  * should grammaticality be viewed a gradient
* how do you sample judgments?
  * **acceptability** not quite the same, experimental subjects misled by interpretability
  * eliciting grammaticality != blindly probing the elephant
  * minimally difference sentences such as changing gender agreement, then feed through LM
    * precise statement of what NNs can and can't do
    * can't even predict grammaticality
  * round-trip MTL of grammatical sentences for generating ungrammatical strings
  * grammaticality is not acceptability
    * acceptability includes semantic
      * acceptable sentence must be grammatical, but not the other way around.
    * grammaticality is more involved with whether or not a sentence parses (into a parse tree) without regard for interpretability
      * by a human speaker from their competence
* LMs are good at determining interpretability, but not necessarily gramaticality
* just about everything can be assigned a tree by an LM, but may not be grammatical