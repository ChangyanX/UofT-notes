# 2020-11-18

* statistical chart parsing
  * probability of the tree = all probabilities of local trees
  * start off with cfg, look like a production rule, so we can separate out probabilities
    * assume top-down generative, so need to assume lhs was appropriate (pieces have to fit together)
  * does not require top-down, can still be done bottom-up
  * bottoms out at lexical categories
  * P(Start) = 1
  * generative model thinks top-down
* inside-outside algorithm
  * maximum likelihood estimates on an annotated corpus to increase likelihood of different, unannotated corpus
  * step 1: parse unannotated corpus using MLE
  * step 2: adjust params according to expected relative frequencies of different rules in parse trees obtained in step 1
  * found through dynammic programming
  * normalizer (Z)
* statistical chart parsers still only answer yes or no in PTIME
  * recognition not parsing algorithms
* may be exponentially many trees in this formulation
* not calculating probability that input is a sentence, only the probability of one interpretation (tree)
  * probability of an input string is a sentence needs to sum over probabilities of all possible trees
  * LMs can do much more easily
* evaluation method
  * train on part of a parsed corpus
  * test on different part of a corpus
  * best eval of a method would be data likelihood, but we're scoring trees instead of strings, 
    * hard to defend intuition about resultant numbers
    * trees can have a lot in common even if theyre not identitical
  * trees added to the corpus were added by annotators, not really naturally attested
  * PARSEVAL measures compare parser output to known correct parse
    * use 'partial' measure
    * precision and recall scores over pieces of trees
      * precision: fraction of constituents in output that are correct
        * when an edge appears in chart, edge does belong there
        * not many wrong edges
        * "all my edges are right"
      * recall: fraction of correct constituents in output
        * look at correct parse, see how many edges from correct parse are in output
        * "i have all the right edges"
    * choice between being more accurate (precise), or fattening up the tree by adding more edges
    * F-measure: harmonic mean of precision and recall = 2PR/(P+R)
      * not an F-score!!
      * intuition that precision and recall are equally valuable
    * penalize for cross-brackets per sentence
      * contituents in output that overlap two or more correct one
      * [[Nadia] [[smelled] [the eggplant]]]
      * [[[Nadia] [smelled]] [the eggplant]]
      * want to properly close previous brackets, otherwise it is a cross-bracket
    * classifier accuracy score
* improving
  * probabilities are only based on structures and categories
  * turns out words strongly condition which rule is used
  * improve results by conditioning on more factors, including words
  * semantics -- words themselves give us a bit of access to this
* lexicalized grammars
  * head of a phrase: central or key word
  * lexicalized grammar: refine grammar so rules take heads of phrases into account
    * before: rule for NP
    * after: 
      * rules for NP-aardvark-head
      * rules for NP-abacus-head
      * similarly for VP, PP
    * notation: cat(head, tag)
      * NP(aardvark, NN)
      * PP(without, IN)
    * we need something not quite this much lexacalized
* Lexicalized parsing
  * from unlexicalized rules, instead of chopping a tree into component partial trees, chop trees into traversal of tree
  1. lexicalization: consider head word of each node, not just cat
    * P(t) = P(S) * PI_n P(rule(n)|head(n))
    * head(n) is the PoS tagged head word of node n   
    * walk L, walk R, and generate all the probabilities as we walk
  2. head and parent
     * condition on head and head of parent node in the tree 
* bikel 2004
  * can condition on any information available in generating tree
  * avoid sparseness of lexicalization by decomposing rules
    * make plausible independence assumptions
    * break rules down into small steps
    * each rule still parameterized with word/PoS pair
      * S(bought, VBD) -> NP(week, NN) NP(IBM, NNP) VP(bought, VBD)
    * tree walking automaton
  * lexical rules with probability one
    * tag(word, tag) -> word
  * internal rules with treebank based probabilities
    * x - Ln, Ln-1, ..., H R1, ..., Rm
  * X, Li, H, Ri have the form cat(head, tag)
  * when we walk to the left, generate LHS probabilities outside in
  * when walking right, generate RHS probabilities inside out
  * distance measure to capture properties of attachment relevant to current modifier
    * 0-1 RV whether we are at position i-1 or not
  * backoffs
    * tag probability when no data for specified word
    * to complete non lexicalization when necessary
* models 2 and 3
  * model 2: add verb subcat, arguments and adjuncts
  * model 3: gaps and trace identification
    * trace: where in the tree gap was filled
    * espcially important with addition of subcat
  * model 2 outperforms model 1
  * model 3 does a little bit better than model 2
  * rich info improves parsing performance
  * performance tightly linked to particular type of corpus