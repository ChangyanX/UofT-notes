# 2020-09-25

* differences between word-dependency parsing vs phrase structure
  * no non terminal nodes
  * edge are now labeled with functions
    * labels in phrase structure are emergent from configurational properties
  * flat bracketings, instead of highly nested phrase structure tree
  * nice bounds on size
  * non-projective dependencies present some problems
    * right node raising
    * prosodically very long
    *  A woman arrived who was wearing a hat
       *  who was wearing a hat modifies "woman"
  * dependencies allow you to defer discussions
  * complex word-word dependency construction
    * predicative adjectives
      * I ate the fish naked (naked modifies I)
      * I ate the fish raw (raw modifies fish)
    * coordination
      * Pat and Terry sat and laughed
    * semantic role assignment
      * how do we tell these apart
        * the door opened (1 arg)
        * Erin opened the door (2 args)
        * the door opened a crack (2 args)
    * Quantifier scoping
      * dependency parsing can not handle this
    * Temporal interpretation
      * very hard with dependency
* dependency structure
  * don't want different nodes claiming the same child
  * \(i\) is the head of \(j\) is equivalent to \(i\) is the functor of \(j\) (\(j\) is the argument)
* formal conditions
  * \(G\) is weakly connected
    * for every node \(i\) exists \(j\) \(i \to j\) or \(j \to i\)
    * everyone is getting a role from somebody except the root
    * can add a root node to enforce connectedness
  * acyclic
    * hierarchal relationship between functor and args
  * single head
    * every word has at most one syntactic head
    * only one head can do the projection   
  * projective
    * \(i \to j \implies i \to \) ...
    * **arcs can not cross**
    * most theoretical framworks do not assume projectivity
    * non-projective structures needed to account for
      * long distance dependencies
        * not all languages have long distance pc (prepositional condition)
      * free word order
* simple typed dependencies are underspecified
* despite deficiencies, we can tell "who did what to whom"
* Shift-Reduce type algorithms
  * \(n\)-word sentence will have \(n\) graph nodes
  * data structures:
    * Stack \([..., w_i]_s\) of partially processed tokens
    * Queue \([..., w_i]_q\) of remaining input tokens
  * may be words in between the stack and the queue
  * parsing actions 
    * adding arcs \(w_i \to w_j, w_i \leftarrow w_j\) 
      * stack may not get smaller necessarily
    * stack and queue ops
  * LR parsing in \(O(n)\) time
    * restricted to projective dependency graphs
  * Yamada's algorithm
    * Originally made for Japanese (strictly head-final)
      * adjective precede nouns, objects preceded verbs
      * only needed SHIFT and RIGHT actions
      * Adapted for English for adding LEFT action
    * SHIFT:
      * Take word and put it on the stack
      * \([...]_S [w_i, ...]_Q \to [..., w_i]_S [...]_Q\)
    * LEFT:
      * \([...w_i, w_j]_S [...]_Q \to [..., w_i]_S [...]_Q w_i \to w_j\)
    * RIGHT:
      * \([...w_i, w_j]_S [...]_Q \to [..., w_j]_S [...]_Q w_i \leftarrow w_j\)
    * \(O(n^2)\) time
  * Nivre's algorithm
    * notes incomplete
    * SHIFT:
      * Take word and put it on the stack
      * \([...]_S [w_i, ...]_Q \to [..., w_i]_S [...]_Q\)
    * Reduce:
      * Requires \(\exists w_k : w_k \to w_i\)
      * \([..., w_i]_S [...]_Q \to [...]_S [...]_Q\)
      * takes things off without putting anything back into the queue
    * LEFT-ARC:
      * \([...,w_i]_S [w_j, ...]_Q \to [...]_S [w_j, ...]_Q w_i \to w_j\)
      * \(w_i\) needs to be available
      * Requires \(\not \exists w_k : w_k \to w_i\)
    * RIGHT-ARC:
      * \([...w_i, w_j]_S [...]_Q \to [..., w_i, w_j]_S [...]_Q w_i \leftarrow w_j\)
      * Requires \(\not \exists w_k : w_k \to w_i\)
    * \(O(n)\) time
    * Example
      * needs head on one side, and argent on the other
    * some nodes take args on left, some on right
      * need to be able to go all the way out to the rightmost head
      * the longer the gap the more we need to postpone something else
    * search problem:
      * string to a sequence of actions in algorithm to do parsing
      * use statistics to build an oracle
      * oracle can be approximated by classifier
      * classifier trained using treebank data
    * MaxEnt: log-likelyhood models neural networks
* Supervised problem
  * approximate function from parser states (represented by feature vectors) to parser actions
    * predict action of the automaton that produces the labels
  * features
    * tokens:
      * target tokens
      * linear context (neighbours in \(S\) and\(Q\))
      * Structure context (parents, children, siblings in \(G\))
    * attributes
      * word form (and lemma)
        * albatrosses albatross
        * business busy (?)
        * no need to distinguish between suffixes and lemma
          * just use both and its probably close enough
      * PoS and morpho-synactic features
        * if suffix was -es know noun is plural
      * Dependency type (if labeled)
      * Distance between target tokens
* Neural Networks
  * vectors of input that produced vectors of output
  * mediated by a weight matrix \(W\)
  * add a bias vector
  * output can be
    * lienar, single output (linear)
    * linear, multi output (linear)
    * single output binary (logistic)
    * multi output binary (logistic)
    * 1 of k categorical output (softmax)
  * single layer can only infer linear functions
  * if we stack layers, we can infer non linear functions!
  * goal of training
    * given training data, determine model parameters
    * weight matrix from input to hidden layer \(W_{jk}\)
    * weight matrix from hidden layer to output \(W_{kj}\)
    * bias terms for hidden layer
    * bias terms for output layer
  * strategy
    * compute error at output
    * determine contribution of each parameter to the error
      * take differential of error wrt parameter
    * update parameter commensurate with error it contributed
  * inputs and outputs are numerical!!
  * gap degree is approximated by the number of cross overs of the arcs