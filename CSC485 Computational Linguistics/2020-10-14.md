# 2020-10-14

* "boosting"
  * using a seed classifier with heuristics 
    * bootstrapped from 'seeds' from very large corpus
    * heuristics
      * one sense per discourse
      * one sense per collocation
    * supervised classification algorithm to build list, often called "unsupervised" however
  * come up with decision list
    * list entries weighted with LogL (log likelihood)
    * large weight is good
  * learn decision list for each homonym
  * Use supervised training with seed-classifier tagged data
    * look for collocation as features for classification
    * apply new classifier to whole data set, tag new instances
    * optional: apply one-sense-per-discourse when one sense dominantes text
  * Stop when converged
  * Use final decision list for WSD
  * best result: avg 96.5% accuracy on 12 homonoymous words
    * as good or better than supervise algorithm directly on fully labeled data
    * baseline (most frequent sense) was 63.9% accuracy
  * need to build classifier for each homograph
  * one-sense heuristics
  * Not limited to words
    * 3/4 could mean three-quarters or third of april
    * roman numerals chapter VII -> chapter seven
* Word vectors
  * Words are symbols without intrinsic meaning
  * One-hot representation -- one 1 and a lot of zeroes
    * hotel: [0 0 0 0 0 0 1 0 0 0]T
    * motel: [0 0 0 1 0 0 0 0 0 0] = 0
  * Represent a word by means of its neighbours -- contexts
    * "know a word by teh company it keeps"
  * syntactic and semantic patterning is captured
  * LSA
    * Factorize weighted term document or word context matrix
    * Retain only k singular values
    * Group words together
    * **Count** different kinds of document they occur in 
    * documents semantically cohere in ways words not always do
    * linearly transform original space to organize documents
  * word2vec CBOW/SkipGram
    * train word vectors to try to either
      * predict word given its bag of words context
      * predict context word (position independent from center word)
      * predict particular word (not a word, just a unit in a vector) is being cast of probability of occurence within a particular context of a starting word
    * update word vectors until they can do prediction well