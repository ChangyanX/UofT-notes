# 2020-10-21

* CFG backbone is adequate for basic syntactic structure
* Use RNN to break matrices, following syntactic 
  * SU-RNN learns soft notions of head words
* Semantic head
* Without NNs, we look at POS, stemming, syntactic patterns
  * Only accuracy of about 60.1
* MaxEnt, with wordnet, n-grams, morphological features, we get 77.6 accuracy
* If we throw the kitchen sink, and use Support Vector Machine, we get ~82.2 accuracy
  * POS, WordNet, prefixes, morphological features, dependency parse, Levin classes, everything
* Matrix-Vector-RNN
  * model semantic head/mutual dependencies by having a vector and a matrix for every word
  * when we combine 2 words, exchange matrices, instead of using single matrix, multiply exchanged matrix with our own vector
  * 2 possible vector combinations, submitted to W + non linearity to get resultant vector
  * can see meronymous relations
  * largely a function of 2 words!
* Recursive Neural Tensor Network
  * linear combination of several matrices
  * allow both additive and mediated multiplicative interactions of vectors
  * adding and multiplying vectors is a good idea
  * work really well if we feed this result into another classifier
  * like support vector machines, and mv-rnns
* Tree-Structured Long Short-Term memory networks
  * track lots of context
  * use syntactic structures
  * preserve history from previous computations
  * consider how relevant close or far words are to current world context
  * multiply input vector by weight
  * maintain hidden state that is used to mediate current and next cell state
  * gate based,
  * combines necessary inputs with 2 previous cell states, bottom up
  * has pearson correlation of 0.868 
* Can we tell if one piece of text follows from each other (Rich Textual Entainment)
  * Two senators received contributions engineered by lobbyist Jack Abramoff in return for political favours
  * Jack Abramoff attempted to bribe two legislators
  * Not enough evidence to show that one sentence entails the other.
* Natural Logic
  * implementable logic for natural language inference without logical forms
  * this natural logic is true