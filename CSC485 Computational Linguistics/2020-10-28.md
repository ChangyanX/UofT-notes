# 2020-10-28 Agreement features

* some token has to bear some agreement features
  * ex german; der neu**e** Wagen
   * usually first token that is able to show agreement features does it
 * chinese classifiers/counters
 * many languages have other agreements
* gender agreement
   * chichewa has genders for men, women, briges, houses, dimuitives, men inside houses, 12-18 grammatical genders
* inflectional morphology
  * some hungarian verbs has 4096 many inflected forms
  * inflectional morpheme exists to reflect properties in a paradigm
    * does not change 'core' meaning, i.e. unmoving vs moving is not an inflectional morpheme
  * paradigm
    * past, present, etc
  * agree with inflection of other words
* base form
  * form you would find a word in a dictionary
    * assume dictionaries that do not contain inflected forms, e.g. past or plurals
  * each open-class word type has base-form/stem/lemma
  * each occurrence of a word includes inflection by (possibly null) morphological change
    * predicting these inflections is hard
      * industry/industries/industri (chopped off s)
    * the lemma is what is left over after mechanically chopping off endings
    * stem is either lemma or base-forms
  * stemmers and lemmatizers work differently, but aspire to get to the base form
* we need to proliferate non-terminals in order to capture morphological distinctions and agreement
  * replace all NPs, Vs, and VPs?
  * S -> NP VP
  * NP -> you, dog, dogs, bears
  * S -> NP(3s) VP(3s) (third person singular)
  * S -> NP(3p) VP(3p) (third person plural)
  * ...and so on?
  * VP -> V NP
  * V -> washes, wash, washed, is, was, ...
* drawbacks
  * result is really big
  * all the S, NP VPs have the same structure
  * expanding the grammar to account for agreement 
  * featural rules collapse together structurally
  * morphology doesn't really affect syntax
  * all information has to be completely and directly specified
    * we cant say values must be equal for some feature without saying exactly what values
* feature structure
  * separate feature information from syntactic/structural/lexical info
  * a feature structure is a list of pairs (feature-name, feature-value)
  * annotations that keep track of aspects of state, metadata
  * values may be atoms or features
  * can represent syntactic structure
  * can consider syntactic category or word to be a bundle of feature
  * can be recursively embedded
  * some people consider NPs a bundles of features, i.e. NOUN + projection = NP
  * example, these express the same NP 'dogs'
    * N [Num s, Pers 3, Lex dog]
    * dog [Cat N, Num s, Pers 3]
    * [Cat N, Agr [Num S, Pers 3], Lex dog]
      * nested structure
      * (Agr Pers 3) feature path
  * Form an NP by combining a Det and N
    * [Cat NP, Num s, Det [Num s, Pers 3, Lex a], N [Num s, Pers 3, Lex dog]]
  * no longer a CFG due to structure, 'phrase-structure' rule
* What if we annotate CFG rules with feature paths?
  * N -> dog (N Agr) = 3s
  * N -> dogs (N Agr) = 3p
  * V -> sleep (V Agr) = { 1s, 2s, 2p, 2p, 3p } (disjunction)
  * creates one more level of indirection
  * description of a feature structure, not a feature structure itself
* constraint based grammar
  * we can use paths as constraints, typically equational constraints
    * NP -> Det N (Det Num) = (N Num)
    * S -> NP VP (NP Agr) = (VP Agr)
  * projection is the sharing of featuers between head of a phrase and the phrase
    * VP -> V (VP Agr) = (V Agr)
    * head-word itself can be a feature
* What is equality for features?
  * a copy of the value or the same pointer
  * **exact same feature structure object**
* certain feature structures can be compatible -- unified
  * [Cat N, Pers 3, Num s] and [Cat N, Pers 3, Gndr F] can be uninioned without changing existing information
  * compatible but not equal
  * Feature structure X subsumes feature structure Y if Y is consistent with, and at least as specific as X
    * Y extends X
    * Y can add non contradictory features
  * X ⊑ Y iff there is a simulation of X inside Y
  * [Cat N, Pers 3] ⊑ [Cat N, Pers 3, Gndr F], but not the above example where Num and Gndr differ
* some feature structures grow more specific
* some assert some properties
* unification
  * Most general, least informative feature structure subsumed by both
  * X ⋃ Y = Z implies Z is the most general feature structure that subsumes both -- union
* can adapt algorithm to use phrase structure grammar
  * each constituent has associated feature structure
  * FS of new arc is initialized with all known constraints
  * FS of predicted constituent unified with that of completed constituent extending the arc
* eventually we get to a point that we can combine in a sentence
* 
