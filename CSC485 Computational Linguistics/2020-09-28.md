# 2020-09-28

## neural networks
* neural networks are equivalent to TMs, they can emulate any arbitrary function.
* want to bound the range of output between 0 and 1 to make decisions
  * logistic function
* one of k discrete possible outcomes, want to assign weights to each output
* softmax layer: rescales logistic layers (categorical distributions)   
* give supervised access to training set: known input, known output
* adjust computing error and backpropogate error
  * assign blame for the error for weight matrices
  * multiple epochs of training
* designer would choose hyperparameters based on characteristics
  * number of hidden layers
  * hidden layers in each layer
  * learning rate (scaling factor of magnitude of changes in each epoch)
  * regularization coefficient (penalize unwanted answers, follow other constraints)
  * number of outputs
  * type of output (linear, logistic, softmax)
    * sigmoid (bound between 0 and 1), tanh, hard tanh, softsign, Rectified Linear (ReLu)
  * choice of non linearity at output and hidden layer
  * input representation and dimensionality
* Objective functions
  * linear-mean squared error function
  * cross entropy error: weight matrix probability distribution
  * compute a derivative that looks like \(y_k - t_k\) (\(y_k\) is predicted output, for output unit \(k\), and \(t_k\) the corresponding target)
* high level backprop algorithm
  * apply input vector to network and forward propagate
  * calculates activations by multiplying matrix by previous levels of the input
  * add bias
  * apply non-linearity function (for example ReLU)
  * get vector or probability
  * get delta \(\delta_k = o_k - t_k\), where \(t_k\) is known target, and \(o_k\) is output
  * backpropogate deltas to obtain \(\delta_j\) for each hidden unit \(j\).
  * need to evaluate derivatives
    * \(\frac{\delta E}{\delta W_{ji}} = \delta_j z_i\)

## syntax and interpretation
* use NN to guess sequence of operations from string to syntax tree
* interpretation: given a tree, get meaning!
* dependency trees good at "who did what to who"
* things, predicates: entities 
* roles: relations between things and predicates
* syntactic structure helps in
  * determining things and predicates
  * determining mapping of things to roles of predicates
* role != grammatical function
* labels are not exactly roles, they are grammatical functions
* once we have the word, and the grammatical function, we can usually find the role
* The goalie kicked the ball: `kick (agent = goalie, theme = ball)`
  * agent: doers, thing that did the kicking
  * theme: assumption of what things can the action be done on (things that can be kicked)
* syntax <-> interpretation
  * mapping of structure to objects of interpretation
  * Things are generally NPs or sentences
  * Predicates are generally VP, PP, APs
  * What are roles?
    * Joan found the treasure in the garage
      * The "finding of the treasure" is happening at the garage (location)
      * ガレージ**で**宝を見つけた
    * Ken put the ball in the garage
      * The ball wound up in the garage (motion to a place)
      * ボールをがーレジ**に**置いた
  * Parse trees more or less indicate grammatical function
    * subject ~ agent
      * if you know the verb
    * object ~ theme
    * object of PP ~ goal/location/recipient
* mapping used to determine appropriate semantic representation
* subjects usually are in nominative case
* unless we have pronouns, we may not have case in the parse tree
* we dont have enough cases in english to know everything
  * in english, case to grammatical function is not entirely regular, but not 1-1
    * Subject: nominative, subjective
    * Object: accusative/ objective
    * Object of preposition: accusative/objective