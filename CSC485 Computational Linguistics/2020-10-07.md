# 2020-10-07

* VerbNet groups by syntactic behavior (Levin classes)
  * grouping by meaning is a side effect
* FrameNet groups by meaning class (frame)
  * not limited to verb
  * grouping by syntactic behaviour is a side effect
* word sense disambiguation
  * "word token sense disambiguation"
  * most english words have only one sense
    * part of speech ambiguity much more frequent
  * ambiguous words are more frequently used
    * around 21-40% of words in english have several senses
  * some senses more frequent than others
    * can rely on statistical analysis
    * what words occur with different senses
  * often no agreement on proper sense division of words
  * even though the lexicon itself is not that bad, in a corpus we keep coming across ambiguous words
  * don't want sense divisions to be too coarse or too fine grained
  * Consider *distinction*
    * Word senses
      1. fact of being different
      2. the quality of being unusually good
    * What sense is "before the war, shares with Rilke and Kafka the distinction of having origins which seem to escape..."
  * Getting sense-tagged corpora is difficult, but hard to get human baseline for performance
    * human annotators agree 70-95% of the time
  * homograph disambiguation is much easier, so much so it's pretty much a solved problem
  * Algorithms
    * PoS tagged input, baseline algo is to pick most likely sense (or even one at random)
      * 39-62% accuracy
    * Collocations: notice when ambiguous word is in unambiguous fixed phrase
      * private school, private eye
    * One sense per discourse: homonymous word is rarely used in more than one sense in the same text
      * Not true for polysemy
    * Lesk's algorithm
      * assume ambiguous word with well defined senses \(s_1\) to \(s_k\)
      * Sense \(s_i\) of ambiguous word \(w\) is likely to be the intended sense if many of the words used in the dictionary definition of \(s_i\) are also used in the definition of words of the context window.
      * Each sense \(s_i\) of word \(w\), let \(D_i\) be the bag of words used to describe \(s_i\) (unordered, stores number of occurrences, excludes stop words like 'the', 'on', 'a', 'in', etc.)
        * stop words are sense indiscriminant
      * Let \(B\) the bag of words of the dictionary definitions of all the senses of all senses of all words \(v \neq w\) in the context window of \(w\)
        * All the \(D_i\) for all the \(v\) that are not \(w\)
        * Care about sense when it comes to \(w\)
        * Assumes one sense per collocation
        * hope that any word, in any other definition, might somehow be somehow be semantically related to the target sense \(s_k\)
      * Choose sense \(s_i\) that maximizes the overlap between \(D_i\) and \(B\).
      * Many variants of overlap score
        * cosine similarity of vectors (dot product)
  * Context
    * meaning of word in use depends on context
    * circumstantial context
    * textual context
      * complete text
        * assumes one sense per discourse rule
      * sentence, paragraph
      * window of n words
    * words of context also ambiguous
    * one sense per collocation (words occurring together)
      * ambiguities, if you play them off each other you get something unambiguous
      * weaker than one sense per discourse
    * selectional restrictions -- contraints imposed by one word meaning on another
      * some words select more strongly than other
      * see (something) -- weak selection
      * drink (something liquid) -- moderate selection
      * elapse (time) -- strong selection
      * limitations
        * presuppositional negation can negate the restriction
          * "You can't eat good intentions"
          * It's nonsense to say that a book elapsed
          * I am not a *crook* -- not saying that he did not commit a crime, but he is not type of person described as a crook
        * Odd events:
          * Jannene Swift married 50-pound pet rock
        * metaphor
          * "marry the impossible to the inevitable"
          * eventually metaphors become just another sense of the word
      * selectional restrictions only help in about 20% of cases, with about 50% accuracy
      * coarse filter for other methods
  * Simple versions of Lesk achieve 50-60% accuracy. Adding simple optimization yields accuracy of nearly 70%
    * Include example sentences in dictionary definitions
    * Include manually tagged example texts
    * PoS tags on definitions
    * Give extra weight to infrequently occurring words