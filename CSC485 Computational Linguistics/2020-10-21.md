# 2020-10-21

* Natural Language Inference
  * Simple task to define, but we need full complexity of compositional semantics
    * Lexical Entailment
    * Quantification
    * Coreference
    * Lexical and scope ambiguity
    * Common sense knowledge
    * Modals (necessity, sufficiency, possibility)
* MacCartney's logic
  * Implementable logic for natural language inference
  * break down a sentence to a set of comparisons by aligning sentences, look at aligned pairs of words
  * for example, blue jeans => pants
  * Seven possible relations
    * ≡ equivalence (couch ≡ sofa)
    * ⊏ forward entailment (strict) (crow ⊏ bird)
    * ⊐ reverse entailment (strict) (European ⊐ French)
    * negation (exhaustive exclusion) (human ^ nonhuman) 
      * Must be either human or non human, can no be both at the same time
    * | alternation (non-exhaustive exclusion) (cat | dog)
      * Can be another type of animal
    * ‿ cover (non-exclusive exhaustion) (animal ‿ nonhuman)
      * all animals are non-human, non-humans may be animals
    * \# independence (hungry # hippo) 
      * no relation between two items
  * We can combine relations to join
  * Can we make an NN learn to make inferences?
* MacCartney's logic does not have functions, unlike lambda calculus
* Quantifiers
  * Three basic types
    * Quantifiers: some, all, no, most, two, three,
    * Predicates: dog, cat, mammal
    * Negation: not
  * "not most" is not exactly the same as "few"
  * QPP with optional negation on predicate
    * (most warthogs) walk ^ (not-most warthogs) walk
    * (most mammals) move # (not-most (not turtles)) move
      * Not the case that most not-turtles move
    * (most (not pets)) ⊐ (not-most (not pets)) move
  * Quite difficult even for humans
  * Tree based RNTN do very well on these semantics
    * Without lambdas!
* To do NLI on real english, we need to teach NN English nearly from scratch
* Data sources:
  * GloVE/word2vec
  * SICK
    * thousands of examples created by editing and pairing hunreds of sentences
  * RTE
    * hundreds of examples created by hand
  * DenotationGraph
    * noisy examples constructed fully automatically
    * estimated to be 73% correct
  * Stanford NLI corpus
    * 600k examples written by mechanical turk
* Distributional learning has been a success
  * can not use analogical reasoning
  * we want meanings of larger units, calculated compositionally
  * ability to do natural language inference
* Top-down parsing
  * Can I take these rules and match them to input
  * Initial goal is S
  * Repeatedly look for rules that decompose/expand current goals
    * S may decompose to NP and VP
  * Eventually get to goals that look at input
    * NP may decompose to det noun
  * Succeed iff entire input stream is accounted for
* Bottom-up paring
  * Can I take this input and match it to these rules
  * Try to find rules that match possible PoS of input words
  * and then rules match the constituents formed
  * Succeed iif the entire input is eventually matched to S
  * May have trees of infinite height that are grammatical
  * Statistical approach (spectral radius < 1) implies convergence
  * Shift-reduce parser
* Neither top-down or bottom-up search exploit useful idiosyncracies that CFG rules alone or together have
  * Problems:
    * recompute constituents and common prefixes
    * want to avoid having to pop off good decisions just to deal with one bad decision
      * In a top-down parser, a bad decision may create a RH daughter that has too many expansions
  * Solutions:
    * Keep track of completed constituents, partial match of rules
    * Chart parsing
  * Avoid problems with blind search
* Chart Parsing
  * Use a **chart** and an **agenda**
  * Agenda is the list of constituents that need to be processed
  * Chart memoizes work, obviates repetition
    * Flexible queue
    * Similar ideas: Well-formed substring table, CKY parsing, Early parsing, dynamic programming
  * Assign numbers to the spaces between words from 0 to n
    * last dot always equals number of words in the input
    * all edges labeled with non-terminal nodes only
      * can have big or small spans
      * have 2 kind of edges
        * inactive arcs 
          * somebody has already derived the value of the arc
          * labeled with the left non-terminal of the grammar rule that derived it
          * directed, always L-to-R
        * active arcs
          * partially built constituents
          * hypothesis, work in progress
          * label is grammar rule used for arc