# 2020-10-16

* factorizing LSA should not converge but because we have numerical floating point stability it eventually converges
* We get 2 vector space matrices
  * one for docs, one for words
  * get singular values (keep \(k\)) to scale matrices to form A
* in word2vec CBOW, arrows are reversed compared to LSA
  * trying to predict "input" word
  * select vectors of prediction, run through softmax to predict "input"
  * word2vec captures dimensions of similarity as linear relations
  * we may need to exclude input...
* Is there a social bias in language models?
  * none of the 'reported' answers actually work, we need to exclude some of the inputs to actually work
* SVD not really capable of training on large corpus
* SVD
  * primarily used to capture word imilarity
  * disapropriate importance to small counts
* w2v
  * scales with corpus size
  * inefficient usage of stats
* we can plug SVD into neutral networks for a better result
* how to we distinguish good relations from bad relations?
  * ratios of co-occurents propobabilities
  * consider x = solid, x = gas, x = water
    * Why choose P(x|ice) over P(x|steam) or vice versa for each x?
    * consider P(x|ice)/P(x|steam)
    * since we're in the log domain, we can just subtract
* glove visualizations
  * captures company-ceo
  * capital cities
* word embedding conclusion
  * develop model that can translate meaningful relationship between word-word co-occurrence probabilities into linear reletations

* compositionality
  * just start putting things together in a complex mechanism
  * if we make a mistake, easy for htings to fall apart
  * want some modularity
  * AI requires understanding bigger things at smaller levels
    * break down thing
  * how can we analyze larger units similar in meaning?
    * most of the time we don't, we process text in context
  * consider the two sentences
    * Two senators received contribution engineered by lobbyist Jack Abramoff
    * Jack Abramoff attempted to bribe two legislators
    * how can we tell these two sentences are similar??
  * Represent phrases as vectors
    * extend word vector spaces to phrases
    * Consider
      * the country of my birth
      * the place where I was born
    * We can't just add, we can use compositionality
      * combine vector space embeddings of words according to syntax tree
    * use same strats in image processing
* Tree RNN
  * process vectors not LR, but bottom to top 
  * 'bottom up parser'
  * given any 2 vectors, combine the two vectors using an NN to get a result
  * simple concat tree RNN
    * only single weight matrix
    * we just staple two inputs together
    * no real interaction between input words
    * not adequate for human language composition
    * move weight up and we get a sort of recurrence
  * syntactically untied RNN
    * use syntax to generalize matrix
    * parse inputs with normal syntactic parsers with CFG rules
    * use different weight matrices depending on the grammar rule
* word vectors get around 50-400 dimensions
* 