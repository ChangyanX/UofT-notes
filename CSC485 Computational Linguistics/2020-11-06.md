# 2020-11-06
* supervised vs unsupervised
  * supervised learning -- learn from known, correct answers
    * can overfit to training data
  * unsupervised learning -- no known answer
    * unsupervised for eventual problem, but supervised for the first problem
    * collecting data is relatively cheap
* Three way partitioning of corpus data
  * small development cycle inside development data
* statistical pp attachment
  * input: V, N1, P, N2
  * output: V-attach or N-attach
  * does not cover all PP problems
* hindle & rooth 93
  * corpus: partially parsed news text
  * output: collection of little trees
  * data: [v|-, n?, p?] triples
    * hyphen means too early in the sentence to know
  * bootstrapping
    * label unambiuous cases as N- or V- attach
    * iterate until nothing changes
      * compute lexical association score for each triple from labelled data
      * label attachment of any new triple whose score is over threshold
    * deal with leftovers (random assignment)
  * compute LA score or fail
    * calculated by log-likelihood ratio of verb- and noun- attach
    * log_2 P(V-attach p | v, n) / P(N-attach p |v,n)
    * cant get these probabilities directly, data is too sparse
    * estimate them from data that we can get
    * decomposed into product P(V-attach p | v) P(null |n) ~ P(N-attach p |n)
      * Noun does not really matter unless we have a null for the corresponding verb
      * nouns are a "method of last resort"
      * verbs basically do not know
    * log of a ratio resembles divergence 
    * results: 80% accuracy
    * 66% accuracy justby saying 'noun'
    * large amount of unlabeled data in tandem with statistic knowledge
* transformation based learning
  * make classification decision, which wont be very good
  * go back and fix mistakes
  * learns sequence of rules to apply to each input item
  * flip attachment decision (from v to n1) or vice versa
  * (v, n1, p, n2) is w1, and (v, n1, p, n2) is w2
  * all rules apply in order which they are learned
* brill & resnik 
  * unlabelled text -> state labeller -> labelled text [attaches everything, may not be correct] -> learner <- truth -> labeller
  * repeat from truth to labelled text
  * grab first rule that matches conditions
  * learner uses diffs between truth and labellex text to select new rule, and applies it
  * output is transformations, ordered list of rules to apply to new data
  * keep learning new rules instead of stopping
  * advantages: rules are readable, can build bias in initial annotation
  * disadvantage: supervised, no strength of preference, memory intensive
  * importance to cl:
    * successfully general method for non-statistical learning from annotated corpus
    * based on popular and easily modified tagger
  * 