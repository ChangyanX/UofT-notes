# 2018-06-14

A *statistic* is any value computed from a set of observations. Then a *sample statistic* is a value calculated from a sample of size \(n\).

Examples of statistics
  
* mean, median, mode (measures of central tendency)
* sample standard deviation, sample variance (related, but not the same as theoretical variance)
* maximum, minimum


If, for example,  the theoretical mean \(\mu\) is the average height of all Canadians, then if we collect the height of 100 Canadians, we have a sample mean \(\overset{-}{x}\)

Every sample of size \(n\) is likely to yield different outcomes. Hence a sample statistic is a random variable. The probability distribution of this random variable is called a **sampling distribution**; the probability distribution of individual values.

For example, patient wait times may follow an \(Exp(4)\) distribution. However, the total wait time of 10 people is a gamma distribution \(\Gamma(10, 4)\). Then the sampling distribution of \(n = 10\) is the gamma distribution.

## Central Limit Theorem

The **sampling distribution** of a sample mean \(\overset{-}{X} = \frac{\sum_{i=1}^n X_i}{n}\) for large \(n\) can be approximated by a normal distribution **regardless of the individual distribution of \(X_i\)**, as long as \(X_i\) are independent and identically distributed.

Let \(X_1, X_2, ..., X_n\) be independent and identically distributed (i.i.d) random variables with \(E(X_i) = \mu\) and \(V(X_i) = \sigma^2 < \infty\)

Then 

\[
Y_n = \frac{\overset{-}{X_n} - \mu}{\sigma / \sqrt{n}}
\]

where

\[
\overset{-}{X} = \frac{\sum_{i=1}^n X_i}{n}
\]

converges in distribution to \(N(0, 1)\)

Alternatively, if \(X_1, X_2, ..., X_n\) are i.i.d RVs with the same mean and variance (identically distributed), then

\[
\overset{\_}{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
\]

where \(\sigma^2 = V(X_i) = V(\frac{X_1 + X_2 + ... + X_n}{n})\)

Notice that \(\frac{\sigma^2}{n} = V(\overset{\_}X) = \frac{1}{n^2} n \sigma^2\) and as \(n \to \infty\), the variance of \(\overset{\_}{X}\) tends to 0.


### CLT Examples

> The amount of water consumption \(W\) in litres per individual per day is on average 1.7L, with a variance of 0.1L^2.
>
> If 40 people are randomly selected and surveyed, what is the probability that their average water consumption is more than 2L?

Notice that we are not given the distribution of \(W\).\(\mu = 1.7\), and \(\sigma^2 = 0.1\) Notice that \(\overset{\_}W = \frac{W_1 + W_2 + ... + W_40}{40}\)

Hence we have the distrbution of \(\overset{\_}{W} \sim N(1.7, \frac{0.1}{40})\). The question then proceeds as a normal distribution question.

> Find an interval that includes the average consumption of 40 intervals with a probability of 95%. Interpret the interval.

We can find the upper tail, lower tail, or the central tail of 95%. However, to find the most common consumption levels, we only consider the interval between the negative and positive extremes.

FIrst we find the distance in standard deviations of 95%, then reconvert to a average consumption value..

We have \(\mu = 1.7\), and we need to find \(a \leq \mu \leq b\). We simply set \(\mu = 0\), find using the table the z-scores for 97.5% and 2.5%

Then the 97.5th percentile is .1.96 standard deviations above the average, and 2.5th percentile is -1.96 standard deviations away from the average.


95% of values hence lie within 1.96 standard deviations of the mean, hene we have the interval \([\mu - 1.96 \sigma, \mu + 1.96\sigma]\)

### Bernoulli and CLT

The central limit theorem only works for **quantitative** observations, or when the RV is a numerical value over a sample space. The only exception is **bernoulli variables**.


Specifically if \(Y \sim Bernoulli(p)\), then the sample mean is a **sample proportion**. 

#### Bernoulli Examples

Consider an outcome of 20 surveys where the success is 1 and failure is 0

1, 1, 1, 1, 0, 0, 1, 1, ...1, 0....

Assuming this corresponds to a sample mean of 15 out of 20, or 75% of 20 people, we can apply the central limit theorem. Since we know the underlying distribution, we have that the expected value is \(p\), and variance is \(p(1-p)\), then by the CLT, if \(n\) is sufficiently large, \(\overset{\_}{Y} = \hat{p} \sim N(p, \frac{p(1-p)}{n})\)

### (p -values)

[stufff about p values here]

## "Sufficiently large \(n\)"

If \(X\) is quantatative, then \(n \geq 25-30\) is a good rule of thumb. In some cases \(n \geq 15\) is large enough. The more uncommon the distribution, the larger the sample size should be to come up with a good approximation. 

If \(X\) is qualitative, then \(n\) should be sufficiently large such that \(np \geq 10\) and \(n(1-p) \geq 10\). If \(p \approx 1\) or \(p \approx 0\), then the sampling is very skewed for small \(n\).

## Normal Approximation to Binomial Distribution
Since \(Binomial(n, p) = \sum^n Bernoulli(p)\) if we have a sample proportion \(\hat{p} = \frac{Y_1 + Y_2 + ... + Y_n}{n}\), then \(X = n \hat{p}\)

Then the count \(X \sim N(np, np(1-p))\) provided \(n\) sufficiently large (see qualitative guidelines on "Sufficiently large n"). A continuity correction is required, since \(X\) is discrete.

**Always check that \(n\) is large enough**

## Sample Mean within x of the true mean (Example)

GIven \(\sigma^2 = 1\) and \(\bar{X}\) a sample mean with \(n = 36\), we want to find the probability that the sample mean will be within 0.3 of the true mean \(\mu\).

To do so, we expand the notation \(P(|\bar{X} - \mu \leq 0.3) = P \left(\frac{-0.3}{\sigma \div \sqrt{n}} \leq Z \leq \frac{0.3}{\sigma \div \sqrt{n}} \right)\)

where we have \(\sigma = 1\) and \(\sqrt{n} = \sqrt{36}\). Hence this is equal to \(P(-1.8 \leq Z \leq 1.8)\) where \(Z\) is the standard normal distribution.

Notice that we are dividing the target threshold (0.3) by the standard deviation of \(\bar{X}\), \(\sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}\)

## Difference of two sample means

This can be expanded to find the difference of two i.i.d sample means \(\bar{X}\) and \(\bar{Y}\).

Take the data in the following table as an example

|Value|\(\bar{X}\)|\(\bar{Y}\)|
|--|---|---|
|\(n\)|50|100|
|\(\sigma^2\)|0.01|0.02|

Then the difference \(\bar{X}  - \bar{Y}\) is also normally distributed with mean \(\mu_x - \mu_y\) and variance \(\frac{\sigma^2_x}{n_x} + \frac{\sigma^2_y}{n_y}\)

Taking advantage of this, to find the the probabilith that the difference between sample means will be within 0.05 of the true difference \(\mu_x - \mu_y\) we apply the same technique as above, keeping in mind we are adjusting for the standard variance.

\[P(|(\bar{X} - \bar{Y}) - (\mu_x - \mu_y)| < 0.05) = P\left(- \frac{0.05}{\sqrt{\frac{\sigma^2_x}{n_x} + \frac{\sigma^2_y}{n_y}}} \leq Z \leq \frac{0.05}{\sqrt{\frac{\sigma^2_x}{n_x} + \frac{\sigma^2_y}{n_y}}}\right)\]


Plugging in values for the mean and \(n\) we have

\[
P\left(- \frac{0.05}{\sqrt{\frac{\sigma^2_x}{n_x} + \frac{\sigma^2_y}{n_y}}} \leq Z \leq \frac{0.05}{\sqrt{\frac{\sigma^2_x}{n_x} + \frac{\sigma^2_y}{n_y}}}\right) =
P\left(- \frac{0.05}{\sqrt{\frac{0.01}{50} + \frac{0.02}{100}}} \leq Z \leq \frac{0.05}{\sqrt{\frac{0.01}{50} + \frac{0.02}{100}}}\right) = 
P(-2.5 \leq Z \leq 2.5) = 0.9876
\]

Where \(Z\) is the standard normal distribution.


