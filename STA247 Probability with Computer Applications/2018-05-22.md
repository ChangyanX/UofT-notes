# 2018-05-22

## CDF Examples
Consider a game of rolling a pair of dice, and observing the sum. Let \(S\) be the sum of the two dice, and \(S = {2, 3, ..., 12}\). (Refer to slides 5.1)

Note that the outcomes in \(S\) are not equally likely. Note the PMF for the sum of two dice

|S|2|3|4|5|6|7|8|9|10|11|12|
|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
|\(P(S=s)\) (Out of 36)|1|2|3|4|5|6|5|4|3|2|1| 
|\(P(S \leq s\) (Out of 36)|1|3|6|10|15|21|26|30|33|35|36|

Note that the CDF adds up to \(\frac{36}{36} = 1\). 

Suppose now that the game has a payout. If the sum is greater than 8 then you will win $2, and you win $0.50 otherwise. 

Let \(W\) be the amount won in a game of 2 dice rolls.

|W|$2|$0.50|
|-|-|-|
\(P(W=w)\)|\(P(S > 8) = \frac{10}{36}\)|\(P(S \leq 8) = \frac{26}{36}\)|

Notice that \(P(S > 8)\) is the complement of the CDF for 8..

The CDF Graph is  non-decreasing step function. For all \(a < b\), \(F(a) \leq F(b)\). There are certain values of \(x\) that will not occur. For example, \(P(x = 4) = 0\).
If a value occurs, then the CDF graph will jump. The CDF is defined to be right-continuous. The height between "jumps" of the CDF at a value \(x\) is the probablity\(P(X = x)\).

**def'n** *Expected Value*
The expected value of a discrete random variabe \(X\), denoted \(E(X)\), with PMF \(f(x)\) is the value expected in the long run through infinite trials. The expected value of \(X\) is given by 

\[
E(X) = \sum_{x \in X} x \cdot f(x) = \mu
\]

The expected value is a weighted average, where the sum \(x\) is the sum of all possible outcomes, and \(f(x)\) is the weighting for each outcome.

The expected value of a function is similarly

\[
E(g(x)) = \sum_{x \in X} g(x) \cdot f(x)
\]

where \(g(x)\) is a real valued function of a discrete random variable \(X\), for a PMF \(f(x)\).

If we do not know the specific distribution for a value \(y = g(x)\), but have a PMF of \(y\), \(F(y)\), we can calculate the expected value as 

\[
E(y) = \sum_{y \in Y} yF(y)
\]

**def'n** *Variance*
The **variance** of a discrete random variable \(X\) with PMF \(f(x)\) is the average squared variability of \(X\) from the expected value. On average, what is the *deviation* of every value of the random variable from the expected value. It is calculated as 

\[
V(X) = E(X - \mu)^2 = \sum_{x \in X} (x - \mu)^2 \cdot f(x) = \sigma^2
\]

If we do not square the deviation, then the deviation will cancel out, as some values of \(x\) will be negative, and some positive. By squaring the deviation, we gain a positive distance to see the deviation Note that \(E(X) = \mu\) is a constant number, which is the expected value of the random variable.

**def'n** *Standard Deviation*
The *standard deviation* of a random variable \(X\) is defined as 

\[
SD(X) = \sigma = \sqrt{\sigma^2} = \sqrt{V(X)}
\]

## Variance examples.

Consider the following PMF

|X|0|1|2|3|4|5|
|-|-|-|-|-|-|-|
|P(X = x)|0.05|0.10|0.15|0.25|0.35|0.10|

The expected value \(\mu\) can be calculated \(\mu = 0.1 + 2 \times 0.15 + 3 \times 0.25 + 4 \times 0.35 + 5 \times 0.1 = 3.05\)

To calculate \(\sigma^2 = (0-3.05)^2 \times 0.05 + (1 - 3.05)^2 \times 0.10 + ... + (5 - 3.05)^2 \times 0.10 = 1.7475\) 

Hence the standard variation of this distribution is \(1.3\). 

The denser a plot of a distribution against an average \(\mu\), the less its distribution.

## Properties of expectation

For a fixed value \(a\), \(E(a) = a\). Similarly, \(E(X + a) = E(X) + a\), and \(E(Xa) = aE(X)\). Following these rules, for a constant \(b\), \(E(aX + b) = a\mu + b\). As well, \(V(a) = 0\), and \(V(a + X) = V(X)\). If we multiply the variance however \(V(aX) = a^2 \cdot V(X)\). Similarly, \(V(aX + b) = a^2 \cdot V(X)\).

For two random variables \(X\) and \(Y\), \(E(X + Y) = E(X) + E(Y)\). However, unless \(X\) and \(Y\) are independent, \(E(XY) \neq E(X) \cdot E(Y)\). Similarly, \(V(X + Y) \neq V(nX) + V(Y)\) unless \(X\) and \(Y\) are independent.

## A formula for variance

\[
E((X - \mu)^2) = E(X^2) - E(X)^2
\]

which can be calculated from the definition of variance. **Note**, \(E(X^2) \neq E(X)^2\). Instead, we calculate it by 

\[
E(X^2) = \sum_{x \in X} x^2 \cdot f(x) 
\]

## Bernoulli Trials
A **bernoulli trial** is a random experiment consisting of exactly one trial involving two possible outcomes, often called a *success* or *failure*. Let \(X\) be the outcome of a Bernoulli trial where \(X = 1\) if the outcome is success, and \(X = 0\) if the outcome is a failure. \(p\) is the probability of success and \(q = 1 - p\) to be the probability of failure. Then the PMF for this trial is thence \(p(X = x) = p^x \cdot (1 - p)^{1-x}\). Then the expected value is equal to \(p\), and the variance is \(p \cdot (1 - p)\).

## Binomial experiment
A **binomial experiment** consists of \(n\) **independent** Bernoulli trials, each with two outcomes. The probability of success \(p\) is fixed for each trial. If \(X\) is the random variable representing the number of successes arising from \(n\) trials, then \(X\) has a binomial distribution and a probability of success \(p = X \thicksim Bin(n, p)\), with PMF

\[P(X = x) = {n \choose x} p ^ x \cdot (1-p)^{n-x}\]


