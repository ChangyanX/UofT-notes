# 2018-06-05

 * Gamma distribution
 * Exponential distribution
 * Poisson distribution
 * Normal distribution

## Gamma distribution
* 2 parameters
* \(\alpha > 0\) - shape parameter
* \(\beta > 0\) - scale parameter.

For a continuous RV \(X\), \(X \sim \Gamma(\alpha, \beta)\) if it has PDF 

\[
f(x) = \begin{cases} 
  \frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha-1}e^{-\frac{x}{\beta}}&, x \geq 0 \\
  0 \end{cases}
\]

where \(\Gamma(\alpha)\) is defined as 
\[
\Gamma(\alpha) = \int_0^\infty x^{\alpha -1} e^{-x}dx
\]

For \(\alpha \in \mathbb{Z}^+\), we have that \(\Gamma(\alpha) = (\alpha - 1)!\)
For \(\alpha > 1, \Gamma(\alpha) = (a - 1)\Gamma(a - 1)\) 
Notice that \(\Gamma(0.5) = \sqrt{\pi}\).

The **sum** of \(n\) independent \(Exp(\theta)\) random variables \(T = X_1 + X_2 + ...  + X_n\) is an RV where \(T \sim \Gamma(n, \theta)\).

Since the CDF of \(\Gamma(\alpha, \beta)\) has no closed form, it is difficult to compute these probabilities when \(\alpha, \beta \notin \mathbb{Z}^+\)

### Uses
The gamma distribution is often used to measure waiting times. Notice that \(\Gamma(\alpha = 1, \beta) = Exp(\lambda = \beta)\)

### Examples
> Two systems have life length in hours \(X_1, X_2 \sim Exp(\theta = 450)\). These systems operate **independently** with \(X_2\) taking over immediately when \(X_1\) fails. What is the PDF and EV for the total length?

Intuitively, the lifetime of both devices is 900. Let \(T = X_1 + X_2\) be the total lifetime. Since both are exponentially distributed with the same parameter, \(T \sim \Gamma(\alpha = 2, \beta = 450)\). Hence \(E[T] = \alpha 
\beta = 900\), and the variance \(V[T] = \alpha \beta^2 = 2 \cdot 450\)

> What is the probability that the total life length in hours exceeds 1000?

\(P(t > 1000) = \int_{1000}^{\infty} f(y) = \frac{1}{450^2} \int_{1000}^\infty ye^{-\frac{y}{450}} dy\)

> #### Integration by parts
> From the classic definition of integration by parts
> \(\int u dv = uv - \int v du\)
> Notice that we differentiate \(u\) and integrate \(dv\).
> Hence we can differentiate \(u\) \(n\) times until we get 0, and integrate \(dv\) by the same \(n\) times.
> Then for each value in a table, we add the cross product (diagonally), alternating between adding and subtracting.
> This method works only for \(C^n\) functions where \(n\) is small.
Integrating by parts we arrive at ...

> If it takes \(X \sim \Gamma(\alpha= 3, \beta = 2)\) minutes to conduct a maintenance check. If it takes 20 minutes to perform a check, does this seem out of line with prior experience?

Note that we have \(E[X] = \mu = \alpha \beta = 6\), and \(\sigma = \sqrt{V[X]} = \sqrt{\alpha \beta^2} = \sqrt{12} = 3.5\)

20 minutes is thence quite unusual, but we can quantify exactly how unusual a time of 20 minutes by finding \(P( x \geq 20)\) to find out how unlikely it is. Notice that \(P(x \geq 20)\) is not as useful for this case, since the mean is less than 20. Thence this \(P(x \geq 20)\) is unable is separate between the lesser probabilities.

Finding

\[P(x \geq 20), X \sim \Gamma(3, 2) = \int_{20}^\infty \frac{x^2 e^-{\frac{x}{2}}}{\Gamma(3) 2^3} dx\]

\[\frac{1}{16} \cdot -2x^2e^{-x/2} ... | ^\infty_{x=20} = 0.0028\]

Since a repair time of 20 minutes has a probability of 0.28%, this outcome is extremely unusual.

We can also quantify with Chebyshev's inequality. We have \(P(|X - \mu|) < k\sigma \geq 1 - \frac{1}{k^2}\), where \(\mu = 6\) and \(\sigma^2 = 12\). Solving for \(k\), we have that \(\frac{X-\mu}{\sigma} = \frac{20 -6}{\sqrt{12}} = 4.04\) standard deviations. By taking the complement \(P(|X - \mu| \geq 4.04\sigma) \geq \frac{1}{4.04^2} = 0.06\) indicates that the probability of 20+ minutes is at most 6%. 

In either case, since the probability is so small, the maintenance took an abnormal amount of time.

## Normal Distribution
A normal distribution has **location** \(\mu\) and **scale** \(\sigma^2 > 0\). For a continuous RV \(X\), \(X \sim N(\mu, \sigma^2)\) if it has PDF

\[
f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-{\frac{(x - \mu)^2}{2 \sigma^2}}}
\]

You can not integrate this distribution.

The expected value \(E[X] = \mu\) is the center of the distribution. A larger variance \(\sigma^2\) is the spread of the distribution. The larger the variance, the flatter and wider the bell the curve forms.

The normal distribution is symmetric about \(\mu\) in other words, \(P(X \geq \mu) = P(x \leq \mu) = 0.5\).

The **standard normal** \(Z \sim N(0, 1)\) has mean 0 and variance 1.

The CDF is found with \(\Phi(x)\), as it does not have a closed form solution. Since the general \(Z\) table CDFs, we have to **standardize** a normal distribution. 

Any normal random variable \(X \sim N(\mu, \sigma^2)\) can be **transformed** into the standard normal \(Z \sim N(0, 1)\) by the formula 

\[
Z = \frac{X - \mu}{\sigma} \sim N(0, 1)
\]

Intuitively, we are discussion how many standard deviations the standard normal \(Z\) is away from the mean. for an arbitrary normal RV \(X\)

### Usage
For any singular inequality, use the normal distribution table and complements. For inequalities in between, for example 
\(P( -1.51 < Z < 0.88) = P(Z \leq 0.88) - P(Z \leq -1.51)\)

### Examples
> Fasting diabetics have blood glucose levels \(G \sim N(106, 64)\). Find the percentage of diabetics that have blood glucouse levels exceeding 127 (mg/100ml)


\[
P(G > 127) = P(\frac{G - 106}{\sigma} > \frac{127 - 106}{8}) = P(Z > 2.63) = P(Z < -2.63)
\]

Hence we find that 127 mg/100ml is 2.63 standard deviations more than 106. We can use symmetry or complements to find the probabilities to the table.

#### Continuity correction
If the average number of births are approximately distributed \(B \sim N(58, 25)\) (5 standard deviations).

> What is the probability that there will be at least 65 births in a randomly chosen day
Since we are asking what there will be at least, we apply a continuity correcttion

\(P(B > 65) \approx P( \geq 64.5)\).
Since 64.5 is the smallest number that would be rounded up to 65. If we were asked, at most 65 births, we would apply a continuity correction of 65.5

For the probability \(P(30 \geq B \geq 60) \approx P(29.5 \geq B \geq 60.5)\). For exclusive probability \(P(30 < B < 60) \approx P(30.5 < B < 59.5)\)

> How many babies are born in the top 5% of busiest days?

We want to find the 95th percentile. Since *Z-scores* are the number of standard deviations from the mean, we can work backwards. Since 95% is exactly in the middle of two published Normal distribution values, we have Z scores as \(Avg \{1.64, 1.65\} = 1.645\). 95% of births are less than 1.645 standard deviations from the mean, hence the top 5% busiest days have \(\mu + 1.645 = 66.225\) babies.

The top 5% will deliver more than 66 babies.

Notice that for a normal RV, the 50th percentile is \(\mu\).

### Empirical rule
The empirical rule gives an approximation for probabilities on certain intervals.

\(P(|X - \mu|) \leq \sigma) \approx 68\%\)
\(P(|X - \mu|) \leq 2 \sigma) \approx 95\%\)
\(P(|X - \mu|) \leq 3 \sigma) \approx 99.7\%\)

Approximately 68% lie within 1, 95% lie within 2, and 99.7% lie within 99.7%.

### Linear combinations of normal RV

If we have 2 RVs \(X \sim N(\mu_1, \sigma^2_1), Y \sim N(\mu_2, \sigma^2_2)\), then \(W = aX + bY\)

Then \(E[W] = E(aX = bY) a\mu_1 + b\mu_2\), \(V[W] = a^2\sigma_1^2 + b^2 \sigma^2_2\) (if \(X\) is independent of \(Y)\)

Then 

\(W \sim N(E(W), V(W)) \iff X, Y \sim N\)


## Moment generating function

A **moment** of an RV are the expected values of powers itself. \(E(X^k)\) is the \(k^{th}\) moment of \(X\).

Since variance is \(E(X^2) - E(X)^2\), it is a function of the first and second moment, but is not a moment itself.

For a function \(g(t) = e^{at}, a \in \mathbb{R}\), we have that \(g^{(n)}(t) =a^n e^{at}\) and thus \(g^{(n)}(0) = a^n\) 

Hence the moment generating function is defined as 

\[
M(t) = E[e^{tX}] = \begin{cases} \int_{x \in X} e^{tx} f(x) dx & \text{for continuous RV} \\ \sum_{x \in X} e^{tx} f(x) & \text{for discrete RV}\end{cases}
\]

If \(M(t)\) exists, and is differentiable in the neighbourhood of \(t = 0\), then the moments of \(X\) can be generated by \(E[X^k] = M^{(k)}(0)\). Moment generating functions are **unique**. Two distributions that have the same MGF are the same. 

If an RV \(X\) has MGF \(M_x(t)\), and \(Y = aX +b\), then \(Y\) has MGF \(M_y(t) = e^{tb}M_x(at)\). Similarly, if \(X, Y\) are independent, then \(M_{X + Y} = M_X(t)M_Y(t)\)
