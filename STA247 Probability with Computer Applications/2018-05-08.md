# 2018-05-08

## Probability vs. Statistics

> *Probability* is a measure of chance. *Statistics* makes inferences based on past events. 

> *def'n* **Random Experiment**
> 
> The possible outcomes of the experiment are known, but we do not know the outcome before the experiment has been performed. We know that the outcome will follow a probability structure.
>
> *Example*
> Rolling a pair of dice, observing resulting pair of numbers.
> Drawing a 5-card hand from a deck of cards
> Fair coin flip

The outcome of a **Random Experiment** is not *fixed*.

> *def'n* **Sample Space**
> 
> The set of all possible outcomes from a random experiment, denoted \(\Omega\) or \(S\). The sample space depends on the **outcome of interest**.
> 
>*Example*
> Tossing a coin twice would be...
> \(\Omega = \{ ... \}\)
> The desired outcome was not defined, so we can not specify the sample space. 
> However, if we are describing the outcome of the top of the coin, we can say \(\Omega = \{HH, HT,TH,TT\}\), or total number of flips, we have a sample space of \(\{0 \leq x \leq 100 | x \in \mathbb{Z}\}\).


> *def'n* **Event**
> An event is a subset of our sample space.

> *def'n* **Event Complement**
> The complement of an event \(A\) is the event consisting of outcomes that are **not** in \(A\). Usually denoted \(A^c\), \(A'\), or \(A^-\)

> *def'n* **Event Union**
> The union of two events \(A\) and \(B\) is the event consisting of outcomes in \(A\) and \(B\). Notice that \(A\cup A^c = \Omega\).

> *def'n* **Event Intersection**
> The set of outcomes in both \(A\) and \(B\). Notice that \(A \cap A^c = \emptyset\).

> *def'n* **Mutually Exclusive**
> Two events \(A\) and \(B\) are **mutually exclulsive** if events cannot both occurred. If \(A\) occurs, then \(B\) cannot occur. In a diagram, \(A \cap B = \emptyset\).

> *def'n* **Independent Events**
> Two events \(A\) and \(B\) are independent if the occurance of one event **does not affect the occurence** of the other in any way. 

For example, for a single coin toss, if a coin lands heads, then it cannot at the same time lands tails, this is a mutually exclusive event. However, the next coin toss does not depend on the previous coin toss. Mutually exclusive events must be **dependent**. The occurrance of \(A\) changes the probability of \(B\) to 0. 

> **Example 1.**
> *Flipping a fair coin three times, observing the outcome of each coin facing up*
> \(\Omega = \{HHH, HHT, HTH, HTT, TTT, TTH, THT, THH\}\). 
> Note: For an arbitrary number of coin tosses \(n\), the size of sample space\(|\Omega_n| = 2^n\).
> 
> *Flipping a fair coin 3 times and measuring the number of 2 tails*
> Based on the sample space above, there is a \(\frac{3}{8}\) chance. However, if this coin was not fair, there would be outcomes that are more or less likely,
>
> *Flipping a fair coin 3 times and measuring the number of tails*
> \(\Omega = \{0, 1, 2, 3\}\) However, since the outcomes are not equal we know \(P(\text{3 tails}) < P(\text{2 tails})\).
> 
> *Number selection of a Lotto 649 Ticket*.
> The rules are 6 numbers from 1 to 49. Hence we have \(|\Omega| = 6^{49}\)
> \(\Omega = \{(a, b, c, d, e, f) ... \}\).


> **Example 2.**
> Consider two difference dice. Define the following events.
> a. \(A\) = Tossing doubles.
> \(A = \{(1,1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6)\}\)
> b. \(B\) = Tossing such that the sum of the two dice is 8.
> \(B = \{(2, 6), (3, 5), (4,4),(5,3),(6,2)\}\). Notice that \((4, 4)\) is not duplicated because changing the order of what dice rolled a 4 does not change the outcome. 


*DeMorgan's Laws*
For two events \(A\) and \(B\).
* \((A \cup B)^c = A^c \cap B^c\) Neither \(A\) nor \(B\) is equal to not \(A\) and not \(B\)
*  \((A \cap B)^c = A^c \cup B^c\) Not both \(A\) and \(B\) is equal to not \(A\) or not \(B\).

More generally, we have
* \((\bigcup^n_{i=1} A_i)^c = \bigcap^n_{i=1} A_i^c\)
* \((\bigcap^n_{i=1} A_i)^c = \bigcup^n_{i=1} A_i^c\)

## Introduction to Probability

> *def'n* In a random experiment with sample space \(\Omega\), the **probability** of an event, denoted \(P(A)\) is a function that assigns to event \(A\) a numerical value that measures the chance that \(A\) will occur.

**axioms of probability functions**

1. \(P(A) \geq 0\)
2. \(P(\Omega) = 1\)
3. For a set of disjoint (i.e mutually exclusive events \(A_1, A_2, ..., A_n\) in \(\Omega\), \(P(\bigcup^n_{i=1} A_i) = \sum^n_{i=1} P(A_i)\).

For two events \(A\) and \(B\), note that \(B = (B \cap A) \cup (B \cap A^c)\). Then by the third axiom, \(P(B) = P(B\cap A) + P(B \cap A^c) \implies P(B\cap A^c) = P(B) - P(B\cap A)\).
Similarily, for \(A \cup B = A \cup (B \cap A^c)\). Hence, \(P (A \cup B) = P(A) + P(B \cap A^c) = P(A) + P(B) - P(B \cap A)\). If \(A\) and \(B\) are disjoint, we have \(P(B \cap A)\). = 0 hence , \(P (A \cup B) = P(A) + P(B)\).

We can extend this for multiple events, for the **inclusion-exclusion principle**. 