# 2018-05-24

## Negative Binomial Distribution

*Models number of failed attempts before a fixed number of successes is achieved*.

Let \(X\) be the number of failures before the \(r^{th}\) success, and \(p\) a **fixed** probablity of success. Then the PMF

\[
P(X = x) = {x + r - 1 \choose r-1} p^r (1-p)^x
\]

This distribution has \(E(X) = \frac{r(1-p)}{p}\), and variance \(V(X) = \frac{r(1-p)}{p^2}\)

### Example

The probability that a total of 10 people are surveyed before 3 people are found in May is the probability of finding two May babies out of \(10\), then the ways that those can be arranged \(10 \choose 2\), then the fixed probability of one may baby.

## Geometric distribution

Let \(X\) be the random variable representing the number of **failures** before the first success, and \(p\) a **fixed** probability of success. Then 

\[P(X = x) = p(1-p)^x\]

This distribution has \(E(X) = \frac{1-p}{p}\), and variance \(V(X) = \frac{1-p}{p^2}\)

In other words, we have a negative binomial distribution with \(r = 1\). 

The geometric distribution is memoryless. Mathematically, \(P(X \geq j + k | X \geq j) = P(X \geq k)\). Intuitively, the probability that something will happen given a prior event, is the same as that event happening in the first place.

Note that the number of trials is **not fixed** in these distributions.

## Hypergeometric distribution

Used to calculate the probability of selecting \(k\) succsses out of \(n\) selections, when the pool of selections \(N\) is small. We want to see the probability of \(k\) successes in \(n\) trials, which is similar to the binomial distribution, but the trials in the binomial case must be independent, and the probability of each success is fixed.

In the hypergeometric distribution, the trials are not independent, and the probability of success is not fixed. When the pool \(N\) is large relative to \(n\) is small, the hypergeometric distribution approximates the binomial distribution. 

Then 

\[
P(X = x) = \frac{k \choose x}{N - 1}
\]
### Example

Out of 5000 items, 10% are defective. Find the probability of observing at lease one defective item in a selection of 10.

* There are 500 defective items, and 4500 non defective items.
* We are going to sample 10 objects.
* Let \(D\) be the number of defective items. Want \(P(D \geq 1) = 1 - P(D = 0)\)
* \(1 - \frac{4500 \choose 10}{5000 \choose 10} = 0.652\).

Out of 50 items, what is the chance?
* There are 5 items defective now.
* 45 of them are ok
* \(1 - \frac{45 \choose 10}{50 \choose 10} = 0.684\)

But notice if we use a binomial distiribution 

\(1 - {10 \choose 0}(0.9)^{10} = 0.651\), is close to the hypergeometric distribution for large \(N\) but not small \(N\).

## Markov's Inequality
Let \(X\) be a non-negative random variable with expected value \(E(X)\). Then for any \(a > 0\),

\[P(X \geq a) \leq \frac{E(X)}{a}\]

Notice that the distribution of \(X\) is not specified. This states that the probability of \(X\) at least \(a\) is less than the average value over such \(a\).

> **Proof**
>  \(E(X) = \sum_{x \in X} x \cdot P(X = x)\).
> \(= \sum_{x < a} x \cdot P(X = x) + \sum_{x \geq a} x \cdot P(X = x)\)
> \(\geq \sum_{x \geq a} x \cdot P(X = x) \geq a \sum_{x \geq a} P(X = x) = a \cdot P(X \geq a)\)
> Hence \(E(X) \geq a \cdot P(X \geq a)\). DIviding both sides we get \(P(X \geq a) \leq \frac{E(X)}{a}\)
> \(\blacksquare\)

## Chebyshev's Inequality
Let \(X\) be a random variable with expected value \(\mu\) and \(\sigma^2 \in \mathbb{R}\) its variance. Then for any \(k \in \mathbb{Z}^+\), then

\[
P(|X - \mu| < k\sigma) \geq 1 - \frac{1}{k^2}
\]

In English, the probability that \(X\) will be within \(k\) standard deviations is at least \(1 - \frac{1}{k^2}\).

> **Proof**
> From *Markov's Inequality*, we have that \(P(X \geq a) \leq \frac{E(X)}{a}\).
> If we have a probability \(P(|x - \mu| \geq k\sigma) = P((x - \mu)^2) \geq k^2 \sigma^2 \leq \frac{E[(x-\mu)^2]}{k^2\sigma^2} = \frac{\sigma^2}{k^2 \sigma^2}\)

Notice that the probability estimate only depends on how \(X\) is from \(\mu\) in terms of number of standard deviations. If the std. deviation is large, then it is more likely to see larger values than for a smaller std. deviation. 

### Example
The daily production of motors averaged 120 with \(\sigma = 10\). What can be said about the fraction of days where the production level is between 100 and 140 units?

Let \(X\) be the number of motors produced. We want \(P(|X- 120| < 20)\), and 20 is 2 standard deviations. Hence by Chebyshev's inequality, \(P(|X - 120| < 20) \geq 1 - \frac{1}{4} = 0.75\). Thus, at least 75% of days will production fall between 100 and 140 units.

What is the shortest interval certain to contain at least 90% of the daily production levels?

If we know \(P(|x - 120| < 10k) \geq 0.9\), then we know that \(0.9 = 1 - \frac{1}{k^2}\). Solving for k yields \(k = \sqrt{10}\). Now we know that at least 90% of the values lie between square root 10 standard deviations. 

### Poisson Distribution

How do we model number of successes over a continous interval? For example number of accidents in an intersection over one week?

Arbitrarily dividing one week into \(n\) subintervals such that \(P(\text{1 accident in subinterval} = p) \land P(\text{0 accidents in subinterval) = 1-p} \land P(\text{2 or more accidents in a subinterval}) = 0\)

Each subinterval is independent. We also include the restriction that the average number of accidents is constant, and equal to \(\lambda\).

Let \(A\) be the random variable denoting the number of accidents during 1 week. We have \(n\) trials with probability \(p\) fixed from trial to trial.

SInce the mean number of accidents is constant, \(E[A] = np = \lambda\), and \(p = \frac\lambda n\). To find the probability of \(A\), we consider the binomial distribution as \(n \to \inf\), and \(p \to 0\).

We have, following a lengthly derivation, for a discrete random variable \(X\) and \(\lambda\) the mean number of occurances per unit interval

\[
P(X = x) = \frac{\lambda^xe^{-\lambda}}{x!}
\]

X has expectation \(E(X) = \lambda\), and variance \(V(X) = \lambda\)

### Examples
An area of forest has on average 6 trees per 100 sq. m. Since events are 'reasonably independent', and there can't be 2 trees in a spot, we can model this roughly with a Poisson distribution.

What is the probability of having at least 1 tree 100 sq. m. area? Let \(T\) be the number of trees. Then \(T \sim Pois(1, \lambda = 6)\)

\(P(T = 1) = 1 - P(T = 0)\) Since Poisson is not discrete, we must count indirectly.

\(1 - \frac{e^{-6}6^0}{0!} = 0.9975\)
